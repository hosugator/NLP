{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 데이터 분류\n",
    "\n",
    "자연어 처리의 일반적인 응용 프로그램 중 하나는 텍스트 데이터를 분류하는 것입니다. 분류는 다양한 형태로 수행할 수 있습니다: 트윗이 특정 주제와 관련이 있는지 여부를 분류할 수 있으며, 특정 리뷰가 긍정적 또는 부정적으로 인지도 분류할 수 있습니다.\n",
    "\n",
    "오늘은 NLP를 사용하여 NLP 데이터를 분류하는 기술을 실습하겠습니다! \n",
    "먼저, 데이터를 수집, 분석 및 처리할 것입니다. 단어 가방(Bag of words)과 tf-idf 모형을 사용할 것입니다.(데이터 획득 단계에서 수행 한 활동을 기업하십니까?) 이러한 과정은 텍스트를 숫자로 변환합니다.\n",
    "그 다음 우리가 만든 단어 벡터를 사용하여 기계 학습 알고리즘을 사용하여 데이터를 훈련시키고, 분류 작업을 수행하는 모델을 만듭니다.\n",
    "\n",
    "시작해 봅시다!\n",
    "\n",
    "첫 번째 작업에서는 트윗 데이터 모음을 사용하여 트윗이 자연 재해를 언급하는지 아니면 일반 트윗을 참조하는지 예측합니다.\n",
    "\n",
    "먼저 필요한 라이브러리를 가져옵시다!\n",
    "\n",
    "## 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv 파일 열기\n",
    "트윗 데이터 파일을 가지고 있습니까? 데이터가 없는 경우 모듈 24의 노트북을 참조하여 데이터 세트를 다운로드하는 방법을 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('[Dataset]_Module25_disasters_social_media.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류를 위한 데이터 분석\n",
    "데이터 작업을 시작하기 전에 먼저 데이터의 일부 특성을 살펴보겠습니다. 모듈 24에서 데이터 분석을 통해 데이터 구조를 파악해 보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>778243823</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>778243824</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>778243825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>778243826</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>778243827</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
       "0           0  778243823     True      golden                 156   \n",
       "1           1  778243824     True      golden                 152   \n",
       "2           2  778243825     True      golden                 137   \n",
       "3           3  778243826     True      golden                 136   \n",
       "4           4  778243827     True      golden                 138   \n",
       "\n",
       "  _last_judgment_at choose_one  choose_one:confidence choose_one_gold keyword  \\\n",
       "0               NaN   Relevant                 1.0000        Relevant     NaN   \n",
       "1               NaN   Relevant                 1.0000        Relevant     NaN   \n",
       "2               NaN   Relevant                 1.0000        Relevant     NaN   \n",
       "3               NaN   Relevant                 0.9603        Relevant     NaN   \n",
       "4               NaN   Relevant                 1.0000        Relevant     NaN   \n",
       "\n",
       "  location                                               text  tweetid  userid  \n",
       "0      NaN                 Just happened a terrible car crash      1.0     NaN  \n",
       "1      NaN  Our Deeds are the Reason of this #earthquake M...     13.0     NaN  \n",
       "2      NaN  Heard about #earthquake is different cities, s...     14.0     NaN  \n",
       "3      NaN  there is a forest fire at spot pond, geese are...     15.0     NaN  \n",
       "4      NaN             Forest fire near La Ronge Sask. Canada     16.0     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head() 함수를 사용하면 데이터 세트에서 처음 몇 행을 볼 수 있습니다. 데이터 전체의 행이 확인할 수 있습니까?\n",
    "\n",
    "### 실습: 테이터 프레임의 길이를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10876"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "\n",
    "len(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 볼 수 있는 표제는 무엇입니까? 어느 표제가 중요하다고 생각합니까?\n",
    "\n",
    "### '대상(target)'이란 무엇인가?\n",
    "\n",
    "예측하고자 하는 영역을 '대상'이라고 합니다. 예제의 경우 트윗이 자연 재해와 관련이 있는지 또는 관련이 없는지 여부가 대상입니다. 이러한 값은 `['choose_one']`열에 반영됩니다(위에 항목 참조!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### '레이블(labels)'을 기억하십니까?\n",
    "이 데이터 세트에서 대상의 레이블은 인적 자원에 의해 수동으로 작성되었습니다. 향후 데이터 세트에 대한 작업을 수행할 때 수동으로 레이블을 작성하거나 지원자를 찾아야 할 수 있습니다.\n",
    "\n",
    "이 작업은 일반적으로 노력과 시간 면에서 비용이 많이 드는 작업입니다. 이 작업에 대한 저ㅓㄱ합한 직원을 찾을 수 있는 [온라인 플랫폼](https://www.mturk.com/) 도 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 레이블 검사\n",
    "\n",
    "트윗들이 분류된 카테고리를 살펴봅시다. 이를 위해 해당 열 내에서 고유한 값의 수를 찾을 수 있습니다. 파이썬의 내장 함수 `set()`은 값 목록을 가져와서 총 고유 값을 출력합니다. 어떻게 작동하는지 봅시다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple', 'orange', 'pears'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['apple', 'orange', 'apple', 'orange', 'pears'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 이해할 수 있나요? 5개의 리스트에서 3개의 고유한 값이 있으며, 고유한 값만 출력됩니다.\n",
    "\n",
    "### 실습: 6개의 리스트에 2개의 고유한 값이 있도록 함수를 변경하십시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple', 'orange'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['apple', 'orange', 'apple', 'orange', 'orange','apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드 조각은 자연 재해에 대한 특정 트윗의 관련성 척도인 'choose_one' 열의 값을 나열합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Relevant', 'Relevant', 'Relevant', ..., 'Relevant', 'Relevant',\n",
       "       'Relevant'], shape=(10876,), dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.choose_one.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set() 함수를 사용하지 않고, 이 열에 얼마나 많은 고유한 값이 있는지 추측할 수 있습니까? \n",
    "\n",
    "### 실습: set() 함수를 사용하여 'choose_one' 열의 고유한 값을 찾아보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Can't Decide\", 'Not Relevant', 'Relevant'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "set(df_raw.choose_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choose_one\n",
       "Not Relevant    6187\n",
       "Relevant        4673\n",
       "Can't Decide      16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw[\"choose_one\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일리가 있습니까? 트윗은 자연 재해와는 관련이 있거나 자연 재해와 관련이 없을 수 있으며, 데이터를 표시한 사람들이 트윗이 자연 재해와 관련이 있는지 여부를 결정할 수 없는 경우도 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재로서는 이진 방식(관련성 있음 vs 관련성 없음)으로 예측하는 것만 관심을 갖고 있으므로, '결정할 수 없는('can't decide') 클래스는 폐기합니다. 판다 데이터 프레임에서 [기준을 사용하여 데이터 하의 설정](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html) 하는 방법을 기억하십니까?\n",
    "\n",
    "### 실습: 데이터 프레임을 부분 집합으로 나누고 choice_one 열에 'Can't Decision'이 없는 행만 사용하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>778243823</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>778243824</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>778243825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>778243826</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>778243827</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>10871</td>\n",
       "      <td>778261105</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.7629</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>5675678.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>10872</td>\n",
       "      <td>778261106</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>4234.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>10873</td>\n",
       "      <td>778261107</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>3242.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>10874</td>\n",
       "      <td>778261108</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>457.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>10875</td>\n",
       "      <td>778261109</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>6585.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
       "0               0  778243823     True      golden                 156   \n",
       "1               1  778243824     True      golden                 152   \n",
       "2               2  778243825     True      golden                 137   \n",
       "3               3  778243826     True      golden                 136   \n",
       "4               4  778243827     True      golden                 138   \n",
       "...           ...        ...      ...         ...                 ...   \n",
       "10871       10871  778261105     True      golden                 100   \n",
       "10872       10872  778261106     True      golden                  90   \n",
       "10873       10873  778261107     True      golden                 102   \n",
       "10874       10874  778261108     True      golden                  96   \n",
       "10875       10875  778261109     True      golden                  97   \n",
       "\n",
       "      _last_judgment_at choose_one  choose_one:confidence choose_one_gold  \\\n",
       "0                   NaN   Relevant                 1.0000        Relevant   \n",
       "1                   NaN   Relevant                 1.0000        Relevant   \n",
       "2                   NaN   Relevant                 1.0000        Relevant   \n",
       "3                   NaN   Relevant                 0.9603        Relevant   \n",
       "4                   NaN   Relevant                 1.0000        Relevant   \n",
       "...                 ...        ...                    ...             ...   \n",
       "10871               NaN   Relevant                 0.7629        Relevant   \n",
       "10872               NaN   Relevant                 0.9203        Relevant   \n",
       "10873               NaN   Relevant                 1.0000        Relevant   \n",
       "10874               NaN   Relevant                 0.8419        Relevant   \n",
       "10875               NaN   Relevant                 0.8812        Relevant   \n",
       "\n",
       "      keyword location                                               text  \\\n",
       "0         NaN      NaN                 Just happened a terrible car crash   \n",
       "1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "2         NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "3         NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "...       ...      ...                                                ...   \n",
       "10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "10872     NaN      NaN  Police investigating after an e-bike collided ...   \n",
       "10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "10874     NaN      NaN  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "10875     NaN      NaN  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "         tweetid  userid  \n",
       "0            1.0     NaN  \n",
       "1           13.0     NaN  \n",
       "2           14.0     NaN  \n",
       "3           15.0     NaN  \n",
       "4           16.0     NaN  \n",
       "...          ...     ...  \n",
       "10871  5675678.0     NaN  \n",
       "10872     4234.0     NaN  \n",
       "10873     3242.0     NaN  \n",
       "10874      457.0     NaN  \n",
       "10875     6585.0     NaN  \n",
       "\n",
       "[10876 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "df_rel = df_raw.copy()\n",
    "df_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 10873, 10874, 10875], shape=(10860,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Can't Decide'가 아닌 행만 선택하여 새로운 DataFrame 생성\n",
    "df_filtered = df_rel[df_rel[\"choose_one\"] != \"Can't Decide\"].copy().reset_index(drop=True)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Not Relevant', 'Relevant'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_filtered[\"choose_one\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 프레임을 출력하여 수행한 작업을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>2898</td>\n",
       "      <td>778247714</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>6</td>\n",
       "      <td>8/27/15 15:27</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.8365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>damage</td>\n",
       "      <td>Your Conversation</td>\n",
       "      <td>This real **** will damage a ****</td>\n",
       "      <td>6.290800e+17</td>\n",
       "      <td>413637189.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
       "2898        2898  778247714    False   finalized                   6   \n",
       "\n",
       "     _last_judgment_at    choose_one  choose_one:confidence choose_one_gold  \\\n",
       "2898     8/27/15 15:27  Not Relevant                 0.8365             NaN   \n",
       "\n",
       "     keyword           location                               text  \\\n",
       "2898  damage  Your Conversation  This real **** will damage a ****   \n",
       "\n",
       "           tweetid       userid  \n",
       "2898  6.290800e+17  413637189.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "df_filtered.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 프레임의 크기를 확인하십시오. 감소하였나요? 아니면 그대로인가요?\n",
    "\n",
    "### 데이터 프레임의 크기 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10876"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10860"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "len(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 'text' 열과 'choose_one' 열에만 초점을 맞추겠습니다.\n",
    "\n",
    "### 실습: 'text'와 'choose_one' 열만 사용하도록 데이터 프레임을 부분 집합 취하기\n",
    "\n",
    "이 [문서](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html)에서 레이블(열 제목)을 사용하여 데이터를 선택하는 방법을 참조하십시오!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text choose_one\n",
       "0                     Just happened a terrible car crash   Relevant\n",
       "1      Our Deeds are the Reason of this #earthquake M...   Relevant\n",
       "2      Heard about #earthquake is different cities, s...   Relevant\n",
       "3      there is a forest fire at spot pond, geese are...   Relevant\n",
       "4                 Forest fire near La Ronge Sask. Canada   Relevant\n",
       "...                                                  ...        ...\n",
       "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   Relevant\n",
       "10872  Police investigating after an e-bike collided ...   Relevant\n",
       "10873  The Latest: More Homes Razed by Northern Calif...   Relevant\n",
       "10874  MEG issues Hazardous Weather Outlook (HWO) htt...   Relevant\n",
       "10875  #CityofCalgary has activated its Municipal Eme...   Relevant\n",
       "\n",
       "[10860 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "df_filtered_column = df_filtered[[\"text\", \"choose_one\"]].copy()\n",
    "df_filtered_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered_column.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 [문서](https://chrisalbon.com/python/data_wrangling/pandas_map_values_to_values/) 를 참조하여 관련 트윗의 경우 숫자 1로, 관련 없는 트윗의 경우 0으로 매핑합니다. \n",
    "\n",
    "### 실습: 'Relevant'을 1로, 'Irrelevant' 을 0으로 매핑하여 'relevance'라는 새로운 열에 넣습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = {\"Relevant\": 1, \"Not Relevant\": 0}\n",
    "# 관련 값을 숫자 1에, 관련 없는 값을 숫자 0에 매핑합니다.\n",
    "df[\"relevance\"] = df[\"choose_one\"].map(relevance)  # your code here\n",
    "\n",
    "# otehr way\n",
    "# df[\"relevance\"] = df[\"choose_one\"].replace({\"Relevant\": 1, \"Not Relevant\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10070</th>\n",
       "      <td>@FoxNews let me report it to u people instead ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>@TMFK_CO sounds like a terrible time. I'll be ...</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8482</th>\n",
       "      <td>@TopherBreezy it was my fav too!!! Amazing! Mi...</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>Trump &amp;amp; Bill Clinton collide in best consp...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8749</th>\n",
       "      <td>****_defreitas for me it's Revs Per Minute but...</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text    choose_one  \\\n",
       "10070  @FoxNews let me report it to u people instead ...      Relevant   \n",
       "416    @TMFK_CO sounds like a terrible time. I'll be ...  Not Relevant   \n",
       "8482   @TopherBreezy it was my fav too!!! Amazing! Mi...  Not Relevant   \n",
       "2448   Trump &amp; Bill Clinton collide in best consp...      Relevant   \n",
       "8749   ****_defreitas for me it's Revs Per Minute but...  Not Relevant   \n",
       "\n",
       "       relevance  \n",
       "10070          1  \n",
       "416            0  \n",
       "8482           0  \n",
       "2448           1  \n",
       "8749           0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 한 일을 보세요!\n",
    "- 관심 있는 칼럼만 골라서 13개였던 칼럼을 단 3개로 줄였습니다!\n",
    "- '관련성'을 1로, '관련성 없음'을 0으로 매핑했습니다.\n",
    "\n",
    "이제 텍스트 처리를 진행하고, 단어 가방 모형과 tf-idf를 진행하겠습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "\n",
    "첫 번째 단계는 트윗을 정규화하고 토큰화하는 기능을 작성하는 것입니다.(모듈 24에서 수행하였습니다). 아래에 예시가 있지만, 이전에 배운 기술을 활용하여 개선할 수 있습니다. \n",
    "\n",
    "예를 들어, 금지어를 추가하거나 레밍(Lemming) 또는 스티밍(Stemming)으로 데이터 세트를 줄일 수 있습니다. 데이터를 더 많이 전처리 할수록 모델이 더 좋아질 수 있습니다!\n",
    "  \n",
    "금지어 목록에서 그 단어를 금지하도록 선택한 이유는 무엇입니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습:욕설 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'how', 'are', 'you', 'today', 'it', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "# extract_words(sentence) 함수 정의하기\n",
    "# split => 특수문자 제거 => 금지어 제거 => 소문자\n",
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is']\n",
    "    # this replaces all special chars with ' ' - re.sub() 사용\n",
    "    words = #your code here\n",
    "    # word 중에 ingore_words 가 아닌것만 소문자로 변환 - list comprehension() 방식\n",
    "    words_cleaned = [w.lower() for w in words if w.lower() not in ignore_words] #your code here\n",
    "    return words_cleaned \n",
    "\n",
    "# 테스트해 봅시다!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(#your code here))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습: extract_words 함수에 최소 5개 이상의 금지 단어를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습: 같은 문장에서 새로운 금지어를 시험해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'today', 'it', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "# 테스트해 봅시다!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(#your code here))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'it' 라는 단어가 여전히 존재하고 있다는 것을 알고 있습니까? 왜 그런지 아십니까?\n",
    "\n",
    "### 실습: extract_words 함수에 소문자를 반영하는 기능 추가\n",
    "방법은 [여기](https://machinelearningmastery.com/clean-text-machine-learning-python/) 를 참조하십시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'are', 'he', 'she', 'my', 'you', 'it','how']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # 모든 특수 문자를 ' '로 대체합니다.\n",
    "    # words 에 소문자 변환 적용하기\n",
    "    words = #your code here\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 문장으로 다시 해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'today', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "# 테스트해 봅시다!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋습니다! 스티밍 또는 레밍과 같은 자체 프로세스를 파이프라인에 자유롭게 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 단어 가방 모형\n",
    "이제 텍스트 데이터를 전처리하는 기능이 생겼으니 텍스트 데이터를 숫자로 변환하는 작업을 진행할 수 있습니다. 이를 위한 가장 간단한 방법은 단어 가방 모형 알고리즘을 사용하는 것이다. \n",
    "\n",
    "단어 가방에서 각 단어가 각 트윗에 대해 나타나는 횟수를 계산하고 해당 수를 입력 데이터로 사용합니다. 이 작업은 다음 단계를 통해 수행됩니다:\n",
    "1. 말뭉치에 나타나는 모든 단어의 어휘를 만듭니다(말뭉치는 모든 텍스트 데이터의 모음입니다. 즉 모든 트윗).\n",
    "2. 그 어휘를 벡터로 전환합니다. 예를들어 말뭉치에 500개의 고유한 단어가 있는 경우, 어휘의 벡터의 크기는 500이며 각 위치는 말뭉치의 단어에 해당합니다.\n",
    "3. 각 문서 (트윗)에 대해 모든 단어가 나타나는 횟수를 계산하고 해당 숫자를 벡터에 추가합니다.  이렇게 하면 각 문서에 고유한 길이 500 벡터가 생성되어 문서에 나타나는 모든 단어를 나타냅니다.\n",
    "\n",
    "### 예시 \n",
    "두 개의 문서로 구성된 말뭉치를 고려합니다: \n",
    "1. 'I love NLP', \n",
    "2. 'I love machine learning'. \n",
    "\n",
    "### 어휘 \n",
    "어휘는 다음과 같은 단어로 구성된 길이 5의 벡터가 됩니다: \n",
    "\n",
    "'I', 'love', 'NLP', 'machine', 'learning'.  \n",
    "\n",
    "#### 벡터\n",
    "첫 번째 문장 (1 번)에 대한 벡터는 다음과 같습니다.: \n",
    "[1, 1, 1, 0, 0] 'I', 'love', 'NLP'가 포함되어 있지만 'machine', 'learning'은 포함되어 있지 않기 때문입니다..  \n",
    "\n",
    "2에 대한 벡터를 구성할 수 있습니까?\n",
    "\n",
    "1과 2의 벡터를 결합하면, 단어 가방은 첫 번째 행에 벡터 1이 있고, 두 번째 행에 벡터 2가 있는 배열이 됩니다.\n",
    "\n",
    "이제 이 알고리즘을 구현하겠습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 가방 만들기\n",
    "\n",
    "먼저 데이터 세트에 각 단어가 얼마나 자주 나타나는지 알고 싶습니다. \n",
    "\n",
    "우리는 이것을 사전의 형태로 나타낼 수 있는데, 사전의 형식은 {'word':frequency}이며, 여기서 각 키는 'word'이고 frequency는 단어가 데이터 세트에 나타나는 횟수입니다.\n",
    "\n",
    "사전에 대해 더 알고 싶으시면 [파이써 딕셔너리](https://www.w3schools.com/python/python_dictionaries.asp) 를 참조하십시오.  \n",
    "\n",
    "### 해시 맵\n",
    "\n",
    "이 사전을 해시 맵이라고 하며 문서의 각 토큰을 반복하여 점진적으로 구축 할 수 있습니다.\n",
    "\n",
    "해시 맵에서 토큰을 찾을 수 없는 경우 토큰을 해시 맵에 추가하고 빈도를 1로 설정합니다. 토큰이 이미 있으면 빈도를 1씩 증가시킵니다. \n",
    "  \n",
    "우리는 두 가지 기능에서 이 작업을 수행 할 것입니다. \n",
    "1. 먼저 hash_map이라는 사전과 트윗의 토큰을 가져 와서 토큰의 각 단어로 hash_map을 업데이트하는 map_book이라는 함수를 만듭니다. \n",
    "2. 다음으로, 모든 트윗을 반복할 수 있는 함수(make_hash_map이라고 부를 수 있음)를 만들고, hash_map을 업데이트하는 첫 번째 함수를 호출합니다.\n",
    "  \n",
    "*힌트: \n",
    "`for word in tokens:` 를 사용하여 토큰을 반복하여 사용할 수 있습니다.  \n",
    "`if word in hash_map:`을 사용하여 단어가 해시 맵에 존재하는지 확인할 수 있습니다.  \n",
    "`hash_map[word]`을 사용하여 해시 맵의 계수를 평가할 수 있습니다. 이 단어를 1씩 증가시켜 카운트를 늘립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 map_book 함수입니다. 이해할 수 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어의 빈도를 계산합니다\n",
    "def map_book(hash_map, tokens):\n",
    "    if tokens is not None:\n",
    "        for word in tokens:\n",
    "            # 단어가 존재합니까?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 make_hash_map 함수입니다. 이해할 수 있겠습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_map(df):\n",
    "    hash_map = {}\n",
    "    for index, row in df.iterrows():\n",
    "        hash_map = map_book(hash_map, extract_words(row['text']))\n",
    "    return hash_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전을 다시 정의합니다.\n",
    "\n",
    "모든 트윗의 모든 단어를 사용하여 단어 가방을 구성할 수 있지만 데이터의 양이 많이 때문에 컴퓨터에 많은 부담이 될 수 있습니다. 좋은 해결책은 가장 흔한 단어 몇 백 개 혹은 수천 개만 가져가는 것입니다. 우리는 우리의 사전을 500개의 가장 인기 있는 토큰들로만 구성되도록 재정립할 것입니다.\n",
    "  \n",
    "어떻게 이런 일을 할 수 있을까요? 각 토큰이 있는 사전인 해시 맵과 토큰이 값으로 표시된 횟수를 방금 작성했다는 점을 기억하십시오. hash_map과 최대 어휘를 사용하는 frequent_vocab이라는 함수를 작성하고 최대 어휘에 정의 된대로 가장 인기있는 토큰 목록을 반환합니다(지금은 500으로 설정)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 frequest_vocab 함수입니다. 이해할 수 있겠습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent_vocab 함수를 다음과 같은 입력으로 정의하십시오 : word_freq 및 max_features\n",
    "def frequent_vocab(word_freq, max_features): \n",
    "    counter = 0  # 값 0으로 카운터를 초기화하십시오\n",
    "    vocab = []   # Vocab이라는 빈 목록을 만듭니다\n",
    "    # 단어를 빈도수가 낮은 순서로 사전에 나열합니다\n",
    "    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n",
    "       # 상위(max_features) 단어 수를 얻기 위한 루프 함수\n",
    "        if counter<max_features: \n",
    "            vocab.append(key)\n",
    "            counter+=1\n",
    "        else: break\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험! 위의 함수를 (reverse = False)로 변경하면 어떻게 됩니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_map = make_hash_map(df) #토큰 화 된 데이터 세트에서 해시 맵 (단어 및 빈도) 생성\n",
    "\n",
    "vocab=frequent_vocab(hash_map, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험! max_feature를 100으로 변경하면 어떻게 됩니까?\n",
    "나중에 500 개 이상을 되돌려 놓는 것을 잊지 마십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마침내 우리는 단어 가방을 구축합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d다음과 같은 입력으로 함수 bagofwords를 정의: sentence, words\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence) # 문장/트윗을 토큰화하고 변수 sentence_words에 할당\n",
    "    # 빈도 단어 수\n",
    "    bag = np.zeros(len(words)) # 크기가 len(words)이고 0으로 구성된 NumPy 배열 생성\n",
    "    # 트윗에 토큰이 있을 때 데이터를 반복하고 1의 값을 추가\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag) # 하나의 트윗에 대한 단어 가방 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습: 작성된 텍스트 데이터를 사용하여 기능을 테스트합니다.\n",
    "위의 단어 목록을 보고 샘플 텍스트에 추가할 단어를 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 't co http  in for'\n",
    "bagofwords(text, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 가방의 한 행을 보세요!\n",
    "\n",
    "이제 전체 데이터 세트를 통해 이 함수를 반복하려고 합니다. 어떻게 하는지는 아래를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 가방을 포함하도록 지정된 차원이있는 숫자 배열을 설정합니다.\n",
    "n_words = len(vocab)\n",
    "n_docs = len(df)\n",
    "bag_o = np.zeros([n_docs,n_words])\n",
    "# 루프 함수를 사용하여 각 트윗에 대해 새 행을 추가합니다.\n",
    "for ii in range(n_docs): \n",
    "    # 이전 함수 'bagofwords'를 호출합니다. 입력을 참조하십시오 : sentence, words\n",
    "    bag_o[ii,:] = bagofwords(df['text'].iloc[ii], vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 numpy 배열의 [차원](https://stackoverflow.com/questions/14847457/how-do-i-find-the-length-or-dimensions-size-of-a-numpy-matrix-in-python) 을 알아보십시오. 여러분에게 의미가 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10860, 500)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 전체 빈도, 역 문서 빈도 찾기\n",
    "\n",
    "여기서 우리는 문장/트윗에서 가장 의미있는 단어를 사용하고자합니다 가장 자주 사용되는 단어가 중요하다고 생각하는 것이 의미가 있습니까?\n",
    "\n",
    "우리는 먼저 우리의 단어 가방 안에 있는 단어들을 살펴봅니다. 가장 많이 사용하는 단어 20개를 출력합니다.\n",
    "\n",
    "힌트: hash_map은 각 단어를 키로 사용하는 모든 워드와 해당 빈도를 dict{word: frequency} 값으로 표시한 사전입니다. 이 가장 일반적인 단어에 대해 무엇을 알 수 있습니까?\n",
    "\n",
    "자세한 내용은 '주요 기능' 아래의 [이 문서](https://docs.python.org/3/howto/sorting.html) 를 참조하십시오. 개체의 인덱스를 키로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 7442),\n",
       " ('co', 6799),\n",
       " ('http', 6153),\n",
       " ('in', 2805),\n",
       " ('i', 2486),\n",
       " ('ã', 1633),\n",
       " ('s', 1272),\n",
       " ('for', 1243),\n",
       " ('on', 1236),\n",
       " ('that', 844),\n",
       " ('with', 798),\n",
       " ('by', 768),\n",
       " ('at', 745),\n",
       " ('this', 702),\n",
       " ('https', 618),\n",
       " ('from', 613),\n",
       " ('be', 587),\n",
       " ('was', 554),\n",
       " ('â', 535),\n",
       " ('m', 531)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "sorted(hash_map.items(), key=lambda item: (item[1], item[0]), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key=Lambda 란 무엇입니까? 자세한 내용은 [이 기사](https://stackoverflow.com/questions/13669252/what-is-key-lambda/13669294) 를 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 상위 20개 단어를 참조하십시오. 무엇을 알 수 있습니까?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특성 선택\n",
    "가장 일반적인 20 단어는 트윗에 대한 정보가 거의 없습니다. 그것들은 모든 텍스트에서 자주 발견되는 일반적인 단어들뿐만 아니라 트위터 URL의 잔해입니다. 우리는 그것들을 '중요한 특성'이라고 보기 어렵습니다. 그렇다면 모델을 개선하기 위해서는 가장 빈번한 단어만 보는 것이 아니라 더 많은 것을 해야 할 것 같습니다.\n",
    "\n",
    "일부 문서에는 자주 등장하지만 모든 문서에는 없는 단어들을 찾아봐야 할 것 같습니다. 왜 이것이 합리적이라고 생각합니까?\n",
    "\n",
    "이것은 '총 빈도 역문서 빈도'(total frequency inverse document frequency, 약칭 tf-idf)로 알려진 알고리즘의 기본 개념입니다.\n",
    "\n",
    "TFIDF 수식은 다음과 같습니다: \n",
    "$$w_{i,j}=tf_{i,j}*log(\\frac{N}{df_i})$$  \n",
    "이 공식에서 $ i $는 단어 인덱서이고 $ j $는 문서 인덱서입니다.\n",
    "단어 가방에서 각 행은 문서인 반면, 각 열은 해당 문서에 있는 단어의 빈도입니다. 이는 이미 tfidf($tf_{i,j}$)의 '항 빈도' 부분입니다.\n",
    "\n",
    "### 역문서 빈도\n",
    "\n",
    "이제 다음과 같은 방식으로 이해할 수 있는 역 문서 빈도를 계산하려고합니다: 각 단어에 대해 나타나는 문서의 수를 계산한 다음 해당 숫자의 역 로그를 가져옵니다.\n",
    "\n",
    "idf 벡터를 두 부분으로 구성합니다.\n",
    "1. 먼저 각 단어에 대한 단어 빈도를 만듭니다. \n",
    "2. 다음 문서(N)를 단어 빈도로 나누고 결과 로그를 취합니다.\n",
    "\n",
    "획득 단계에서 우리가 한 일을 기억하십니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트윗 수(numdocs)와 토큰/워드 수(numwords)를 나타내는 변수 2개 초기화\n",
    "numdocs, numwords = np.shape(bag_o)\n",
    "\n",
    "# 위와 같이 TFIDF 수식으로 변경\n",
    "N = numdocs\n",
    "word_frequency = np.empty(numwords)\n",
    "\n",
    "# 단어가 나타나는 문서 수를 계산\n",
    "for word in range(numwords):\n",
    "    word_frequency[word]=np.sum((bag_o[:,word]>0)) \n",
    "\n",
    "idf = np.log(N/word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어 가방(용어 빈도)을 idf와 함께 입력함으로써 tfidf를 완료하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화 tfidf 배열\n",
    "tfidf = np.empty([#your code here])\n",
    "\n",
    "# 트윗에서 반복, 용어 빈도(단어 가방으로 표시)를 idf로 곱합니다.\n",
    "for doc in range(#your code here):\n",
    "    tfidf[doc, :]=bag_o[doc, :]*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10860, 500)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.56037575 0.64356806 0.74842242 ... 0.         0.         0.        ]\n",
      " [0.56037575 0.64356806 0.74842242 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf 배열에 대해 어떻게 설명하시겠습니까? 그것은 10860개의 트윗에서 500개의 토큰 각각의 tfidf 값으로 만들어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비단계 통합코드\n",
    "\n",
    "# 0. 라이브러리 선언 및 데이터 로드하기\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "df_raw = pd.read_csv(\n",
    "    \"[Dataset]_Module25_disasters_social_media.csv\", encoding=\"ISO-8859-1\"\n",
    ")\n",
    "\n",
    "df_filtered_chooseone_cantDecide = df_raw[df_raw[\"choose_one\"] != \"Can't Decide\"].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "df_filtered_textAndChooseone = df_filtered_chooseone_cantDecide[[\"text\", \"choose_one\"]]\n",
    "\n",
    "\n",
    "# 1. 텍스트에서 단어를 추출 : extract_words(sentence)\n",
    "def extract_words(sentence):\n",
    "    ignore_words = [\"a\", \"the\", \"if\", \"br\", \"and\", \"of\", \"to\", \"is\"]\n",
    "    words = re.sub(\"[^\\w]\", \" \", sentence).split()\n",
    "    words_cleaned = [w.lower() for w in words if w.lower() not in ignore_words]\n",
    "    return words_cleaned\n",
    "\n",
    "\n",
    "# 2. 단어의 빈도를 계산합니다. : map_book(hash_map, tokens)\n",
    "def map_book(hash_map, tokens):\n",
    "    if tokens is not None:\n",
    "        for word in tokens:\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# 3. 각 단어의 발생 빈도를 계산한 값으로 해시맵을 만듭니다. : make_hash_map(df)\n",
    "def make_hash_map(df):\n",
    "    hash_map = {}\n",
    "    for index, row in df.iterrows():\n",
    "        hash_map = map_book(hash_map, extract_words(row[\"text\"]))\n",
    "    return hash_map\n",
    "\n",
    "\n",
    "# 4. 상위 max_features 단어를 추출합니다. : frequent_vocab(word_freq, max_features)\n",
    "def frequent_vocab(word_freq, max_features):\n",
    "    counter = 0  \n",
    "    vocab = []  \n",
    "    for key, value in sorted(\n",
    "        word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True\n",
    "    ):\n",
    "        if counter < max_features:\n",
    "            vocab.append(key)\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# 5. 주어진 문장에 대해 단어 가방 벡터를 생성합니다. : bagofwords(sentence, words)\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(\n",
    "        sentence\n",
    "    )\n",
    "    bag = np.zeros(len(words)) \n",
    "    for sw in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == sw:\n",
    "                bag[i] += 1\n",
    "\n",
    "    return np.array(bag)\n",
    "\n",
    "\n",
    "# 6. 텍스트 데이터프레임(df)의 각 텍스트(트윗)를 bagofwords 함수를 사용하여 단어 가방(Bag of Words) 벡터로 변환하는 작업 수행\n",
    "hash_map = make_hash_map(df_filtered_textAndChooseone)\n",
    "\n",
    "\n",
    "# 7. IDF 구하기\n",
    "# 8. TF-IDF 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:39: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:39: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/v9/qpg5js816p79wmppzgfqcz9w0000gn/T/ipykernel_95565/2936122555.py:39: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  words = re.sub(\"[^\\w]\", \" \", sentence).split()  # 모든 특수 문자를 ' '로 대체합니다.\n",
      "/var/folders/v9/qpg5js816p79wmppzgfqcz9w0000gn/T/ipykernel_95565/2936122555.py:39: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  words = re.sub(\"[^\\w]\", \" \", sentence).split()  # 모든 특수 문자를 ' '로 대체합니다.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# 루프 함수를 사용하여 각 트윗에 대해 새 행을 추가합니다.\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_docs):\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# 이전 함수 'bagofwords'를 호출합니다. 입력을 참조하십시오 : sentence, words\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     bag_o[ii, :] = bagofwords(df[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].iloc[ii], \u001b[43mvocab\u001b[49m)\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# bag_o[:2]\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# 7. IDF 구하기\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# 트윗 수(numdocs)와 토큰/워드 수(numwords)를 나타내는 변수 2개 초기화\u001b[39;00m\n\u001b[32m    105\u001b[39m numdocs, numwords = np.shape(bag_o)\n",
      "\u001b[31mNameError\u001b[39m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "### 데이터 준비단계 통합코드\n",
    "# 0. 라이브러리 선언 및 데이터 로드하기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "df_raw = pd.read_csv(\n",
    "    \"[Dataset]_Module25_disasters_social_media.csv\", encoding=\"ISO-8859-1\"\n",
    ")\n",
    "\n",
    "df_choice_one = df_raw[df_raw[\"choose_one\"] != \"Can't Decide\"]\n",
    "df = df_choice_one[[\"text\", \"choose_one\"]].copy()\n",
    "relevance = {\"Relevant\": 1, \"Not Relevant\": 0}\n",
    "df[\"relevance\"] = df.choose_one.map(relevance)\n",
    "\n",
    "\n",
    "# 1. 텍스트에서 단어를 추출 : extract_words(sentence)\n",
    "def extract_words(sentence):\n",
    "    \"\"\"This is to clean and tokenize words\"\"\"\n",
    "    # 특수 문자를 공백으로 바꿉니다.\n",
    "    ignore_words = [\n",
    "        \"a\",\n",
    "        \"the\",\n",
    "        \"if\",\n",
    "        \"br\",\n",
    "        \"and\",\n",
    "        \"of\",\n",
    "        \"to\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"he\",\n",
    "        \"she\",\n",
    "        \"my\",\n",
    "        \"you\",\n",
    "        \"it\",\n",
    "        \"how\",\n",
    "    ]\n",
    "    words = re.sub(\"[^\\w]\", \" \", sentence).split()  # 모든 특수 문자를 ' '로 대체합니다.\n",
    "    words_cleaned = [w for w in words if w.lower() not in ignore_words]\n",
    "    return words_cleaned\n",
    "\n",
    "\n",
    "# 2. 단어의 빈도를 계산합니다. : map_book(hash_map, tokens)\n",
    "def map_book(hash_map, tokens):\n",
    "    if tokens is not None:\n",
    "        for word in tokens:\n",
    "            # 단어가 존재합니까?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# 3. 각 단어의 발생 빈도를 계산한 값으로 해시맵을 만듭니다. : make_hash_map(df)\n",
    "def make_hash_map(df):\n",
    "    # 해시맵을 생성합니다.\n",
    "    hash_map = {}\n",
    "\n",
    "    # DataFrame의 각 행에 대해 반복합니다.\n",
    "    for index, row in df.iterrows():\n",
    "        # 단어의 빈도를 계산합니다.\n",
    "        hash_map = map_book(hash_map, extract_words(row[\"text\"]))\n",
    "\n",
    "    # 해시맵을 반환합니다.\n",
    "    return hash_map\n",
    "\n",
    "\n",
    "# 토큰 화 된 데이터 세트에서 해시 맵 (단어 및 빈도) 생성\n",
    "hash_map = make_hash_map(df)\n",
    "\n",
    "\n",
    "# 5. 주어진 문장에 대해 단어 가방 벡터를 생성합니다. : bagofwords(sentence, words)\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(\n",
    "        sentence\n",
    "    )  # 문장/트윗을 토큰화하고 변수 sentence_words에 할당\n",
    "    # 빈도 단어 수\n",
    "    bag = np.zeros(len(words))  # 크기가 len(words)이고 0으로 구성된 NumPy 배열 생성\n",
    "    # 트윗에 토큰이 있을 때 데이터를 반복하고 1의 값을 추가\n",
    "    for sw in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == sw:\n",
    "                bag[i] += 1\n",
    "\n",
    "    return np.array(bag)  # 하나의 트윗에 대한 단어 가방 반환\n",
    "\n",
    "\n",
    "# 6. 텍스트 데이터프레임(df)의 각 텍스트(트윗)를 bagofwords 함수를 사용하여 단어 가방(Bag of Words) 벡터로 변환하는 작업 수행\n",
    "n_docs = len(df)\n",
    "n_words = len(vocab)\n",
    "\n",
    "bag_o = np.zeros([n_docs, n_words])\n",
    "# 루프 함수를 사용하여 각 트윗에 대해 새 행을 추가합니다.\n",
    "for ii in range(n_docs):\n",
    "    # 이전 함수 'bagofwords'를 호출합니다. 입력을 참조하십시오 : sentence, words\n",
    "    bag_o[ii, :] = bagofwords(df[\"text\"].iloc[ii], vocab)\n",
    "\n",
    "# bag_o[:2]\n",
    "# 7. IDF 구하기\n",
    "# 트윗 수(numdocs)와 토큰/워드 수(numwords)를 나타내는 변수 2개 초기화\n",
    "numdocs, numwords = np.shape(bag_o)\n",
    "# 위와 같이 TFIDF 수식으로 변경\n",
    "N = numdocs\n",
    "word_frequency = np.empty(numwords)\n",
    "# 단어가 나타나는 문서 수를 계산\n",
    "for word in range(numwords):\n",
    "    word_frequency[word] = np.sum((bag_o[:, word] > 0))\n",
    "\n",
    "idf = np.log(N / word_frequency)\n",
    "# idf[:2]\n",
    "\n",
    "# 8. TF-IDF 구하기\n",
    "tfidf = np.empty([numdocs, numwords])\n",
    "\n",
    "# 트윗에서 반복, 용어 빈도(단어 가방으로 표시)를 idf로 곱합니다.\n",
    "for doc in range(numdocs):\n",
    "    tfidf[doc, :] = bag_o[doc, :] * idf\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 기계 학습으로 모델을 훈련시킵니다.\n",
    "드디어 TFIDF 배열이 생겼으니 여러분의 모델을 훈련시키고 예측을 할 때입니다! 우리는 많은 기계 학습 모델을 제공하는 scikit learn 라이브러리를 사용할 것입니다. \n",
    "\n",
    "기계 학습이란 무엇인지 기억하십니까? 시스템이 명시적으로 프로그래밍되지 않아도 자동으로 학습 할 수있는 AI의 응용 프로그램입니다.\n",
    "\n",
    "이제 우리는 지도 학습을 사용할 것입니다. 이것이 무엇인지 기억하시나요?\n",
    "이것은 훈련 세트가 주어지면 특정 시스템을 예측하는 모델을 만들 수 있게 해주는 학습 유형입니다. \n",
    "우리는 텍스트가 재난에 대한 뉴스와 관련이 있는지 여부를 예측하려고 합니다. 이미 트위터에서 텍스트 데이터를 다운로드했으며 데이터에 여러 레이블을 붙였습니다.'relevant', 'not relevant', 'can't decide' 등이 있으며, 이러한 데이터는 우리의 모델을 훈련시키는데 사용될 것입니다. 어떻게 할 수 있는지 알아보겠습니다.\n",
    "\n",
    "먼저 이 작업을 수행하는 데 필요한 라이브러리를 다운로드합니다. scikit learn 라이브러리에는 기계 학습 문제에 사용되는 수많은 유용한 기능이 포함되어 있습니다.\n",
    "\n",
    "## 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #로지스틱 회귀 모형 가져오기\n",
    "from sklearn.model_selection import train_test_split # 데이터를 훈련 및 테스트 세트로 분할\n",
    "from sklearn.model_selection import GridSearchCV # 모델의 가장 적합한 매개 변수를 찾기 위해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1단계. 데이터를 교육 및 테스트 세트로 분할\n",
    "\n",
    "모델을 교육하기 전에 데이터 세트를 2개로 분할합니다: 교육 세트, 테스트 세트\n",
    "\n",
    "우리는 교육 세트에서 모델을 훈련시킨 다음 테스트 세트로 훈련 단계에서 생성 된 모델을 테스트합니다. 이는 테스트가 모델에 '보이지 않는'/처리되지 않은 데이터 세트에서 수행되는지 확인하기 위함입니다.\n",
    "\n",
    "분할의 좋은 시작점은 훈련 세트에 데이터의 80%를, 테스트 세트에 데이터의 20%를 분할하는 것입니다.\n",
    "\n",
    "지금 해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_all과 y_all을 교육 및 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X = tfidf, \n",
    "    y = df[\"relevance\"].values, \n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df[\"relevance\"].values,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드에서 무슨 일이 일어나고 있는지 설명해 주시겠습니까? train_test_split 함수를 사용하여 tfidf 배열과 트윗의 'relevance' 값을 포함하는 초기 데이터 프레임의 일부를 분할하고 있습니다. [Shuffle=True](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) 는 무엇을 의미합니까? \n",
    "\n",
    "작업 중인 데이터 세트에 대해 자세히 알아보겠습니다. 아래 tfidf 및 df['relevance'] 를 출력하여 확인하십시오. 한편, [.value](https://www.geeksforgeeks.org/python-pandas-dataframe-values/) 가 무엇인지 알아보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.56037575 0.64356806 0.74842242 ... 0.         0.         0.        ]\n",
      " [0.56037575 0.64356806 0.74842242 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "10871    1\n",
      "10872    1\n",
      "10873    1\n",
      "10874    1\n",
      "10875    1\n",
      "Name: relevance, Length: 10860, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋습니다! 데이터 세트를 이미 분할했으므로 현재 어떤 데이터가 있는지 살펴보겠습니다. X_train, X_test, y_train 및 y_test 모양을 출력합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8145, 500)\n",
      "(2715, 500)\n",
      "(8145,)\n",
      "(2715,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train 및 x_test를 비교하고 y_train 및 y_test를 비교하십시오. 모양의 차이에 대해 무엇을 알 수 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2단계. 모델 인스턴스(instance) 작성\n",
    "데이터를 분할한 후 모델의 인스턴스(예: 모델을 초기화하기만 하면 됩니다)를 만듭니다. 이 작업에서는 데이터를 분류할 때 유용한 로지스틱 회귀 분석기를 사용합니다. 자세한 내용은 [여기](https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102) 를 읽어보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스(instance) 작성\n",
    "logreg = LogisticRegression(solver = 'liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3단계. 데이터에 대한 모델 훈련, 데이터로부터 배운 정보 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 세트에 모델 적합\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금은 위에 표시된 매개 변수를 이해하는 것에 대해 걱정하지 마십시오. 관련된 모든 매개변수를 모를 경우에도 로지스틱 회귀 분석기를 사용하여 프로젝트를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 모델을 사용하여 테스트 데이터를 기반으로 관련성 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred=logreg.predict(X_test)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred는 무엇을 의미합니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델의 정확도 값을 찾아 모델 성능을 측정하겠습니다. 정확도는 다음과 같이 정의됩니다:\n",
    "\n",
    "올바른 예측의 분율 = 정확한 예측 / 총 데이터 포인트 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7683241252302025\n",
      "Accuracy of logistic regression classifier on test set: 0.768\n"
     ]
    }
   ],
   "source": [
    "# 스코어 방법을 사용하여 모델의 정확성을 얻습니다\n",
    "score = logreg.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대단합니다! 우리는 데이터를 수집하고, 처리하고, 교육 및 테스트 데이터로 나누고, 모델을 교육하고, 모델의 성능을 평가했습니다. 다음으로 이 모든 작업을 한 번에 수행할 수 있는 함수를 정의하겠습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련 파이프라인\n",
    "\n",
    "이 기능은 다음과 같이 구성됩니다:\n",
    "1. 훈련되지 않은 모델, tfidf 배열 및 훈련 대상의 값을 가져온다.\n",
    "2. 무작위로 두 개를 나누어 학습과 테스트 세트를 만든다.\n",
    "3. 학습 세트에 모델을 맞춘다.\n",
    "4. 테스트 세트에 정확도 점수를 출력한다.\n",
    "5. 훈련된 모델을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(rf, X_all, y_all): # 훈련되지 않은 모델, tfidf 배열 및 훈련 대상의 값을 가져온다.\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,shuffle=True) # 무작위로 두 개를 나누어 학습과 테스트 세트를 만든다.\n",
    "    logreg.fit(X_train,y_train) # 학습 세트에 모델을 맞춘다.\n",
    "    print(rf.score(X_test,y_test)) # 테스트 세트에 정확도 점수를 출력한다.\n",
    "    return logreg # 훈련된 모델을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터 세트에 함수를 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver = 'newton-cg')\n",
    "#logreg = LogisticRegression()\n",
    "\n",
    "X_all = tfidf\n",
    "y_all = df['relevance'].values\n",
    "logreg = classify(logreg, X_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 매개 변수 조정(옵션)\n",
    "이 권한을 구현 한 경우 적어도 0.75의 테스트 점수가 있어야합니다. \n",
    "\n",
    "하이퍼 매개 변수를 조정하여 이 점수를 개선해 보겠습니다. 먼저 로지스틱 회귀 분석기의 매개 변수를 살펴보겠습니다. 매개 변수에 대한 자세한 내용은 [여기](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) 를 참조하십시오. 다시 말하지만, 만약 여러분이 지금 이 변수들을 이해하지 못한다고 스스로에게 스트레스를 주지 마세요. 그러한 결과는 더 많은 연습과 읽기를 통해 얻어질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매개 변수는 Scikit-Learn의 GridSearchCV를 사용하여 자동으로 조정할 수 있습니다. 이 함수는 테스트할 값이 있는 매개변수 사전뿐만 아니라 모델을 사용하며 각 매개변수 조합을 테스트하여 최상의 스코어에 대한 최적 조합을 찾습니다. [여기](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.) 에서 자세히 알아보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 하이퍼 파라미터를 정의하십시오.\n",
    "parameters = {'C':[0.001, 0.01, 0.1, 1, 10], 'tol':[0.0001, 0.001, 0.01], 'max_iter':[100, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(logreg, parameters, cv=3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.cv_results_를 사용하여 초기 결과를 볼 수 있다.\n",
    "print(clf.best_params_, clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 결과를 보십시오. 떨어지는 것을 알아차리셨나요? 왜 이것이 그렇다고 생각합니까?\n",
    "\n",
    "모델을 무작위로 분할하기 때문에 정확도가 다를 수 있습니다. 우리가 할 수 있는 것은 모델 피팅을 여러 번 반복하고 평균적인 정확도 결과를 얻는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 파이프라인 구축\n",
    "이 시점에서 트윗을 예측할 수 있도록 훈련되고 최적화된 모델이 준비되어 있습니다. 이제 이 모든 것을 하나로 묶어 예측을 위한 파이프라인을 구축합니다.\n",
    "\n",
    "이 기능은 다음과 같습니다:\n",
    "1. 트윗을 문자열 형식으로 취한다.\n",
    "2. 해당 트윗이 관련이 있는지 여부에 대한 예측을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_predictor(tweet):\n",
    "    # your code here\n",
    "    word_vector = bagofwords(tweet, vocab) # 단어 가방 변수를 설정합니다.bagofwords 함수를 기억하십니까?\n",
    "    word_tfidf = word_vector*idf # tfidf값 찾기\n",
    "    prediction = logreg.predict(word_tfidf.reshape(1, -1)) # 트윗이 자연재해와 관련이 있는지 없는지 예측\n",
    "    results = {1:'Relevant', 0:'Not Relevant'} # 잠재적인 결과를 포함하는 집합을 만듭니다.\"Relevant\" 및 \"Not relevant\" 태그를 변경할 수 있습니다.\n",
    "    print(results[int(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1 = 'When the earthquake happened (Nepal) we were the last intl team still there; in a way we were 1st responders'\n",
    "tweet2 = 'NLP is fun and I learnt so much today.'\n",
    "twitter_predictor(tweet1)\n",
    "twitter_predictor(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습: 트윗을 직접 작성하고 모델이 트윗을 분류할 수 있는지 확인해 보십시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 올바른 결과를 제공할 수 있습니까? \n",
    "\n",
    "모델 성능을 향상시키기 위해 무엇을 할 수 있다고 생각하십니까?\n",
    "\n",
    "이전에 상위 500 개 토큰 만 사용한다는 것을 기억하십니까? 우리가 더 많은 토큰을 선택하거나 더 적은 토큰을 선택하면 어떻게 될 것이라고 생각하십니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다! 이제 자신만의 기계 학습 NLP 모델을 구축했습니다.\n",
    "  \n",
    "# 5. NLP 분류 과제!\n",
    "TFIDF가 자연어 데이터 분류를 하기 위해 정규화된 단어 가방을 사용하는 기본 사항을 배웠으니 이제는 여러분의 기술을 테스트할 때입니다!\n",
    "\n",
    "## 감정 분석\n",
    "자연 텍스트 분류의 중요한 응용은 _감정 분석(sentiment analysis)_에 있습니다. 감정 분석은 특정 주제에 대한 작가의 태도를 확인하기 위해 텍스트 조각에서 의견을 분류하는 과정입니다.\n",
    "  \n",
    "이 도전과제에서는 [imdb](https://www.imdb.com/)의 영화 리뷰를 분류합니다. 데이터는 이미 두 개의 .pkl 파일로 저장되었습니다(현재는 파이썬을 사용하여 읽을 수있는 파일 형식으로 이해하십시오). 하나는 훈련 데이터용이고 하나는 테스트 데이터용 입니다.\n",
    "\n",
    "You will have to process and train your model on the train dataset of movie reviews `df_raw.pkl`, and then report the accuracy of your model on the test move reviews `df_raw_test.pkl`.  \n",
    "영화 리뷰 `df_raw.pkl`의 훈련 데이터 세트에서 모델을 교육한 다음, 영화 리뷰 `df_raw_test.pkl` 테스트 데이터 세트를 이용하여 모델의 정확도를 확인해야 합니다.\n",
    "\n",
    "리뷰가 긍정적인 정서를 가지고 있는지 부정적인 정서를 가지고 있는지 예측하게 될 것입니다. 긍정적인 정서는 1로 분류되고, 부정적인 정서는 0으로 분류됩니다.\n",
    "  \n",
    "이 세그먼트에서는 sklearn에서 제공하는 몇 가지 새로운 기능을 사용합니다.\n",
    "1. 이전에 구축한 함수로 TFIDF를 사용하여 단어 가방을 생성하고 조건을 조정할 수 있습니다. \n",
    "2. 대안적으로, [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) 기능을 이용하여 단어 가방을 만들 수 있습니다. `max_features=5000` 인수를 사용하여 가장 일반적인 상위 5000개의 단어만 선택하여 사용합니다.\n",
    "3. TFIDF로 단어 가방을 변환하는 데 도움이 되는 [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) 를 사용할 수도 있습니다.\n",
    "  \n",
    "이 데이터 프레임에 대한 훈련 및 테스트 데이터 세트는 이미 가져왔습니다. 이전에 배운 기술과 모듈 24에서 배운 기술을 사용하여, 데이터 전처리, 벡터화(단어 가방), 변화(TFIDF 사용), 적합성 등을 적용하여 영화 리뷰의 감정을 예측해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer # 이 기능은 단어 가방을 만들 수 있도록 도와줍니다.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # 이 기능은 단어 가방을 자동으로 정규화합니다.\n",
    "df_raw = pd.read_pickle('[Dataset]_Module25_df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('[Dataset]_Module25_df_raw_test.pkl')\n",
    "\n",
    "\n",
    "## 7번째 줄과 8번째 줄에 오류가 있는 경우, \"imdb\" 폴더에서 파일을 복사하여 노트북과 같은 위치에 붙여 넣으십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시작하려면 테스트 데이터 세트의 샘플을 출력해 보십시오. 열의 이름은 무엇입니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, process your text using the `CountVectorizer` to create your bag of words. \n",
    "이제 `CountVectorizer`를 사용하여 텍스트를 처리하여 단어 가방을 만듭니다. `CountVectorizer()`클래스를 만들고 텍스트와 함께 `.fit_transform()` 메서드를 사용하여 단어 모음을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, \\\n",
    "                             preprocessor = None, stop_words = None, max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df_raw['text'])\n",
    "test_data_features = vectorizer.transform(df_raw_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `TfidfTransformer`를 사용하여 단어 가방을 정규화합니다. 사용 방법은 동일합니다.클래스를 만들고 단어 가방과 함께 `.fit_transform()` 메서드를 인수로 사용하여 TFIDF를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfier = TfidfTransformer()\n",
    "tfidf = tfidfier.fit_transform(train_data_features)\n",
    "tfidf_test = tfidfier.transform(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 변환된 단어 가방을 사용하여 이전과 같이 모델을 훈련하고 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = tfidf.toarray()\n",
    "y_all = df_raw['positive'].values\n",
    "X_test = tfidf_test.toarray()\n",
    "y_test = df_raw_test['positive'].values\n",
    "def classify():\n",
    "    rf = LogisticRegression()\n",
    "    rf.fit(X_all,y_all)\n",
    "    print(rf.score(X_test,y_test))\n",
    "    return rf\n",
    "classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터를 조정하지 않고도 80% 이상의 정확도를 가지고 있어야 합니다. 가능한 테스트 세트에서 최상의 정확도를 얻으십시오. 이 데이터 세트는 자연어 처리의 특징 중 하나이며 많은 데이터 과학자와 엔지니어들의 진입점이다. 다른 사람들이 개발한 더 많은 [솔류션](https://www.kaggle.com/c/word2vec-nlp-tutorial) 을 볼 수 있습니다!\n",
    "\n",
    "## 다음으로는 자신의 리뷰를 입력하는 함수를 만들어 자신의 문장이 긍정적인지 부정적인지를 예측하는 모델을 만들 수 있습니다!\n",
    "\n",
    "# 축하합니다!\n",
    "자연 언어 텍스트 분류기를 만드는 방법을 배웠습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
