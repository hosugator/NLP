{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from numpy import argmax\n",
    "\n",
    "# 데이터 로딩 및 전처리\n",
    "X = []\n",
    "Y = []\n",
    "try:\n",
    "    with open('[Dataset]_Module25_disasters_social_media.csv', 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['text'] and row['choose_one']:\n",
    "                X.append(row['text'].strip())\n",
    "                Y.append(row['choose_one'].strip())\n",
    "except FileNotFoundError:\n",
    "    print(\"파일을 찾을 수 없어 예시 데이터를 사용합니다.\")\n",
    "    X = ['Hi', 'Hello', 'How are you?', 'I am making', 'making', 'working', 'studying', 'see you later', 'bye', 'goodbye']\n",
    "    Y = ['greeting', 'greeting', 'greeting', 'busy', 'busy', 'busy', 'busy', 'bye', 'bye', 'bye']\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data = [d.lower() for d in data]\n",
    "    data = [re.sub('[^a-z\\s]', '', d) for d in data]\n",
    "    data = [d.strip() for d in data]\n",
    "    data = [re.sub(' +', ' ', d) for d in data]\n",
    "    return data\n",
    "\n",
    "X = preprocess_data(X)\n",
    "vocabulary = sorted(list(set(' '.join(X).split())))\n",
    "classes = sorted(list(set(Y)))\n",
    "\n",
    "def encode_sentence(sentence, vocab):\n",
    "    sentence = preprocess_data([sentence])[0]\n",
    "    encoded_vector = [0] * len(vocab)\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            encoded_vector[vocab.index(word)] = 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return encoded_vector\n",
    "\n",
    "X_encoded = [encode_sentence(sentence, vocabulary) for sentence in X]\n",
    "Y_encoded = []\n",
    "for label in Y:\n",
    "    label_encoded = [0] * len(classes)\n",
    "    label_encoded[classes.index(label)] = 1\n",
    "    Y_encoded.append(label_encoded)\n",
    "\n",
    "# 모델 생성 및 훈련\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=len(X_encoded[0])))\n",
    "model.add(Dense(units=len(Y_encoded[0]), activation='softmax'))\n",
    "model.compile(loss=categorical_crossentropy, optimizer=SGD(learning_rate=0.01, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
    "\n",
    "print(\"모델 학습을 시작합니다. 잠시만 기다려 주세요...\")\n",
    "\n",
    "history = model.fit(np.array(X_encoded), np.array(Y_encoded), \n",
    "                    epochs=100, \n",
    "                    batch_size=16, \n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 모델 평가\n",
    "predictions = model.predict(np.array(X_encoded), verbose=0)\n",
    "predicted_classes = [classes[argmax(p)] for p in predictions]\n",
    "true_classes = [classes[argmax(p)] for p in Y_encoded]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"모델의 최종 평가 지표:\")\n",
    "print(classification_report(true_classes, predicted_classes, zero_division=0))\n",
    "\n",
    "# 모델 및 관련 데이터 저장\n",
    "model.save('model.h5')\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump({'vocabulary': vocabulary, 'classes': classes}, f)\n",
    "\n",
    "print(\"\\n모델 학습 및 저장이 완료되었습니다. 'model.h5'와 'data.pkl' 파일이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from keras.models import load_model\n",
    "from numpy import argmax\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 모델 및 관련 데이터 불러오기\n",
    "try:\n",
    "    model = load_model('model.h5')\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    vocabulary = data['vocabulary']\n",
    "    classes = data['classes']\n",
    "except (OSError, FileNotFoundError, KeyError):\n",
    "    print(\"모델 파일이 없거나 손상되었습니다. 먼저 모델 학습 코드를 실행해 주세요.\")\n",
    "    exit()\n",
    "\n",
    "# 텍스트 전처리 및 인코딩 함수 (모델 학습 코드와 동일)\n",
    "def preprocess_data(data):\n",
    "    data = [d.lower() for d in data]\n",
    "    data = [re.sub('[^a-z\\s]', '', d) for d in data]\n",
    "    data = [d.strip() for d in data]\n",
    "    data = [re.sub(' +', ' ', d) for d in data]\n",
    "    return data\n",
    "\n",
    "def encode_sentence(sentence, vocab):\n",
    "    sentence = preprocess_data([sentence])[0]\n",
    "    encoded_vector = [0] * len(vocab)\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            encoded_vector[vocab.index(word)] = 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return encoded_vector\n",
    "\n",
    "# 챗봇 테스트 루프\n",
    "print(\"챗봇이 준비되었습니다. '종료'를 입력하면 멈춥니다.\")\n",
    "while True:\n",
    "    sentence = input(\"문장을 입력하세요: \")\n",
    "    if sentence.lower() == '종료':\n",
    "        print(\"챗봇을 종료합니다.\")\n",
    "        break\n",
    "    \n",
    "    encoded_input = np.array([encode_sentence(sentence, vocabulary)])\n",
    "    prediction = model.predict(encoded_input, verbose=0)\n",
    "    \n",
    "    predicted_class_index = argmax(prediction)\n",
    "    predicted_class = classes[predicted_class_index]\n",
    "    \n",
    "    # 예측된 의도에 따라 답변 출력\n",
    "    if predicted_class == 'Relevant':\n",
    "        responses = [\"이것은 재난 관련 트윗으로 분류됩니다.\", \"재난 관련 정보인 것 같습니다.\", \"긴급한 상황인 것 같군요.\"]\n",
    "    elif predicted_class == 'Not Relevant':\n",
    "        responses = [\"이것은 재난과 관련 없는 트윗으로 분류됩니다.\", \"재난 정보가 아닌 것 같습니다.\", \"일반적인 정보로군요.\"]\n",
    "    else:\n",
    "        responses = [\"이해하지 못했어요.\", \"무슨 말씀이신지 잘 모르겠어요.\"]\n",
    "        \n",
    "    print(random.choice(sentence, responses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
