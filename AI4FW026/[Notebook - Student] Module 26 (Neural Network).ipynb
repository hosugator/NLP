{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망으로 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 노트북에서는 코사인 유사성을 가진 챗봇을 만드는 방법을 배웠습니다. 이제는 신경망을 이용해 어떻게 만들 수 있는지 살펴보겠습니다!\n",
    "\n",
    "훈련 데이터를 만들고 신경망을 훈련시킨 다음 훈련된 모델을 사용하여 챗봇을 만들 것입니다.\n",
    "\n",
    "먼저, 필수 라이브러리를 설치할 것입니다. 라이브러리가 설치되지 않은 경우에만 아래 몇 개의 블록을 주석 해제하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy scipy\n",
    "#!pip install scikit-learn\n",
    "#!pip install pillow\n",
    "#!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 설치\n",
    "\n",
    "우선 이 신경망 구동 챗봇에 필요한 라이브러리를 설치하겠습니다.\n",
    "Keras는 백엔드에서 텐서플로우(다른 하위 레벨 기계 학습 라이브러리) 를 활용하는 기계 학습 라이브러리입니다. 이렇게 하면 우리의 목적을 위해 심층 신경망을 쉽게 배포할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hong/venvs/nlp_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense\n",
    " \n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 입력 훈련 데이터\n",
    "\n",
    "먼저 챗봇에 대한 다음 교육 데이터를 포함하겠습니다.:\n",
    "1. X는 사용자가 입력할 수 있는 다양한 입력을 나타냅니다.\n",
    "2. Y는 입력의 의도를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['Hi',\n",
    "     'Hello',\n",
    "     'How are you?',\n",
    "     'I am making',\n",
    "     'making',\n",
    "     'working',\n",
    "     'studying',\n",
    "     'see you later',\n",
    "     'bye',\n",
    "     'goodbye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ['greeting',\n",
    "     'greeting',\n",
    "     'greeting',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'bye',\n",
    "     'bye',\n",
    "     'bye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비슷한 의도를 가진 여러 개의 다른 문장들이 있다는 것을 주목하십시오. 여기에서는 3개의 의도(greeting, busy, bye)만 있지만 프로젝트에 원하는 만큼 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 챗봇이 작동하는 방식입니다:\n",
    "1. 입력 문장으로부터, 우리는 훈련된 AI 모델을 사용하여 의도를 확인할 것이다.\n",
    "2. 각 의도에 대해, 우리는 준비 된 응답을 가지고있다.\n",
    "\n",
    "예를 들어, 입력의 의도가 인사말에 대한 것임을 확인하면 챗봇에 '안녕하세요'또는 '어떻게 지내십니까?'와 같은 인사말로 응답하도록 요청할 수 있습니다.\n",
    "\n",
    "우리는 기계 학습을 사용하여 입력 문장을 다른 의도로 분류 할 수있는 모델을 만들 것입니다. \n",
    "다음과 같이 만듭니다:\n",
    "\n",
    "1. 문장과 그 의도에 대한 목록을 포함하고 있는 훈련 데이터(위의 X및  Y)를 작성한다.\n",
    "2. 훈련 데이터를 사용하여 분류기를 훈련한다. \n",
    "3. 입력 문장을 벡터화하고 분류기를 사용하여 의도를 결정한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 텍스트 처리\n",
    "\n",
    "평소와 같이 텍스트 처리부터 시작합니다. 그 과정을 기억하십니까?\n",
    "\n",
    "## 3.1 알파벳과 숫자가 아닌 문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alpha_numeric_characters(sentence):\n",
    "    new_sentence = ''\n",
    "    for alphabet in sentence:\n",
    "        if alphabet.isalpha() or alphabet == ' ':\n",
    "            new_sentence += alphabet\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    X = [data_point.lower() for data_point in X]\n",
    "    X = [remove_non_alpha_numeric_characters(\n",
    "        sentence) for sentence in X]\n",
    "    X = [data_point.strip() for data_point in X]\n",
    "    X = [re.sub(' +', ' ',\n",
    "                data_point) for data_point in X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_data(X)\n",
    "\n",
    "vocabulary = set()\n",
    "for data_point in X:\n",
    "    for word in data_point.split(' '):\n",
    "        vocabulary.add(word)\n",
    "\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = []\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    sentence = preprocess_data([sentence])[0]\n",
    "    sentence_encoded = [0] * len(vocabulary)\n",
    "    for i in range(len(vocabulary)):\n",
    "        if vocabulary[i] in sentence.split(' '):\n",
    "            sentence_encoded[i] = 1\n",
    "    return sentence_encoded\n",
    "\n",
    "X_encoded = [encode_sentence(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(Y))\n",
    "\n",
    "Y_encoded = []\n",
    "for data_point in Y:\n",
    "    data_point_encoded = [0] * len(classes)\n",
    "    for i in range(len(classes)):\n",
    "        if classes[i] == data_point:\n",
    "            data_point_encoded[i] = 1\n",
    "    Y_encoded.append(data_point_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 훈련 데이터 및 테스트 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_encoded\n",
    "y_train = Y_encoded\n",
    "X_test = X_encoded\n",
    "y_test = Y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 및 테스트 데이터에 사용하는 데이터 출력 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "print (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train은 무엇을 의미합니까? 위에 표시된 배열을 이해합니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 훈련 데이터를 이용해 신경망을 훈련시키겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1.1653\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1467\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1250\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1040\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0866\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0743\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0673\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0645\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0647\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0662\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0676\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0679\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0668\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0642\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0605\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0562\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0519\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0480\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0446\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0418\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0395\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0375\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0357\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0338\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0319\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0297\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0274\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0250\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0225\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0200\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0176\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0152\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0129\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0106\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0083\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0060\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0038\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0015\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9992\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9969\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9946\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9922\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9899\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9876\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9852\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9829\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9806\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9783\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9759\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9736\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9713\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9690\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9666\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9643\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9619\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9596\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9572\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9549\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9525\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9501\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9478\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9454\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9430\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9407\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9383\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9359\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9335\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9311\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9287\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9263\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9239\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9215\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9191\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9167\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9143\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9119\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9094\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9070\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9046\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9021\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8997\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8972\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8948\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8923\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8899\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8874\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8849\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8824\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8799\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8775\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8750\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8725\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8700\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8675\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8649\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8624\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.859 - 0s 2ms/step - loss: 0.8599\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8574\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8548\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x140b49a30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='sigmoid',\n",
    "                input_dim=len(X_train[0])))\n",
    "model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01,\n",
    "                            momentum=0.9, nesterov=True))\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측 목록 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [argmax(pred) for pred in model.predict(np.array(X_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 평가해 봅시다. 모델에 의한 예측과 테스트 데이터를 비교할 것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 8\n",
      "Total: 10\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == argmax(y_test[i]):\n",
    "        correct += 1\n",
    "\n",
    "print (\"Correct:\", correct)\n",
    "print (\"Total:\", len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 챗봇을 테스트해 보겠습니다! 문장을 입력한 다음 신경망에서 예측되는 클래스를 확인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence\n",
      "goodbye\n",
      "bye\n",
      "Enter a sentence\n",
      "bye\n",
      "bye\n",
      "Enter a sentence\n",
      "How are you?\n",
      "greeting\n",
      "Enter a sentence\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4z/g7p3m_nn6sl2kwjpdy8wf4cm0000gn/T/ipykernel_1374/698300813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a sentence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencode_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             )\n\u001b[0;32m--> 981\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅇㄹ\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print (\"Enter a sentence\")\n",
    "    sentence = input()\n",
    "    prediction= model.predict(np.array([encode_sentence(sentence)]))\n",
    "    print (classes[argmax(prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챗봇을 멈출 수 없다는 것을 알고 있습니까? 나중에 종료 명령을 추가해야 합니다(이전 노트를 참조하여 수행 방법을 확인하십시오.).\n",
    "\n",
    "일단은 위의 중지 버튼(인터럽트 버튼)을 눌러서 챗봇을 중지하면 됩니다.\n",
    "\n",
    "시도해 보세요. 정지 버튼을 누르고 상자에 뭔가를 입력해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 도전과제\n",
    "\n",
    "우리는 성공적으로 대화 의도에 우리의 입력을 매핑하는 신경망을 사용했습니다. \n",
    "여러분의 과제는 대화 의도를 챗봇이 말하는 특정 응답과 연결하는 것입니다. \n",
    "예를 들어, 만약 대화의 목적이 '인사' 라면, 여러분의 챗봇도 인사말을 하도록 하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/hong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# NLTK 데이터 다운로드\n",
    "# 처음 한 번만 실행하면 됩니다.\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>778243823</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>778243824</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>778243825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>778243826</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>778243827</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
       "0           0  778243823     True      golden                 156   \n",
       "1           1  778243824     True      golden                 152   \n",
       "2           2  778243825     True      golden                 137   \n",
       "3           3  778243826     True      golden                 136   \n",
       "4           4  778243827     True      golden                 138   \n",
       "\n",
       "  _last_judgment_at choose_one  choose_one:confidence choose_one_gold keyword  \\\n",
       "0               NaN   Relevant                 1.0000        Relevant     NaN   \n",
       "1               NaN   Relevant                 1.0000        Relevant     NaN   \n",
       "2               NaN   Relevant                 1.0000        Relevant     NaN   \n",
       "3               NaN   Relevant                 0.9603        Relevant     NaN   \n",
       "4               NaN   Relevant                 1.0000        Relevant     NaN   \n",
       "\n",
       "  location                                               text  tweetid  userid  \n",
       "0      NaN                 Just happened a terrible car crash      1.0     NaN  \n",
       "1      NaN  Our Deeds are the Reason of this #earthquake M...     13.0     NaN  \n",
       "2      NaN  Heard about #earthquake is different cities, s...     14.0     NaN  \n",
       "3      NaN  there is a forest fire at spot pond, geese are...     15.0     NaN  \n",
       "4      NaN             Forest fire near La Ronge Sask. Canada     16.0     NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('[Dataset]_Module25_disasters_social_media.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one\n",
       "0                 Just happened a terrible car crash   Relevant\n",
       "1  Our Deeds are the Reason of this #earthquake M...   Relevant\n",
       "2  Heard about #earthquake is different cities, s...   Relevant\n",
       "3  there is a forest fire at spot pond, geese are...   Relevant\n",
       "4             Forest fire near La Ronge Sask. Canada   Relevant"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_choose = df[df['choose_one'] != \"Can't Decide\"][['text', 'choose_one']]\n",
    "df_choose.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # punctuation removal\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "\n",
    "    # unwanted space removal\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # lemmatize\n",
    "    words = text.split()  # split makes list\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "df_choose['text'] = df_choose['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 레이블:\n",
      " 0        Relevant\n",
      "1        Relevant\n",
      "2        Relevant\n",
      "3        Relevant\n",
      "4        Relevant\n",
      "           ...   \n",
      "10871    Relevant\n",
      "10872    Relevant\n",
      "10873    Relevant\n",
      "10874    Relevant\n",
      "10875    Relevant\n",
      "Name: choose_one, Length: 10860, dtype: object\n",
      "인코딩된 레이블:\n",
      " [1 1 1 ... 1 1 1]\n",
      "디코딩된 레이블: ['Relevant' 'Relevant' 'Relevant' ... 'Relevant' 'Relevant' 'Relevant']\n"
     ]
    }
   ],
   "source": [
    "# label encoding\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df_choose['choose_one'])\n",
    "\n",
    "# 인코딩된 레이블과 원래 레이블을 출력하여 확인\n",
    "print(\"원본 레이블:\\n\", df_choose['choose_one'])\n",
    "print(\"인코딩된 레이블:\\n\", y_encoded)\n",
    "\n",
    "# 인코딩된 숫자를 다시 원래 레이블로 변환할 수도 있습니다.\n",
    "y_decoded = label_encoder.inverse_transform(y_encoded)\n",
    "print(\"디코딩된 레이블:\", y_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_encoded\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10860"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "# sequence padding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 학습된 토큰화기를 이용해 훈련 데이터와 테스트 데이터를 정수 시퀀스로 변환\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_api_export_path',\n",
       " '_api_export_symbol_id',\n",
       " 'analyzer',\n",
       " 'char_level',\n",
       " 'document_count',\n",
       " 'filters',\n",
       " 'fit_on_sequences',\n",
       " 'fit_on_texts',\n",
       " 'get_config',\n",
       " 'index_docs',\n",
       " 'index_word',\n",
       " 'lower',\n",
       " 'num_words',\n",
       " 'oov_token',\n",
       " 'sequences_to_matrix',\n",
       " 'sequences_to_texts',\n",
       " 'sequences_to_texts_generator',\n",
       " 'split',\n",
       " 'texts_to_matrix',\n",
       " 'texts_to_sequences',\n",
       " 'texts_to_sequences_generator',\n",
       " 'to_json',\n",
       " 'word_counts',\n",
       " 'word_docs',\n",
       " 'word_index']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fire': 1,\n",
       " 'like': 2,\n",
       " 'im': 3,\n",
       " 'amp': 4,\n",
       " 'get': 5,\n",
       " 'u': 6,\n",
       " 'new': 7,\n",
       " 'via': 8,\n",
       " 'one': 9,\n",
       " 'news': 10,\n",
       " 'dont': 11,\n",
       " 'people': 12,\n",
       " 'time': 13,\n",
       " 'emergency': 14,\n",
       " 'disaster': 15,\n",
       " 'video': 16,\n",
       " 'year': 17,\n",
       " 'would': 18,\n",
       " 'body': 19,\n",
       " 'police': 20,\n",
       " 'building': 21,\n",
       " 'go': 22,\n",
       " 'day': 23,\n",
       " 'still': 24,\n",
       " 'life': 25,\n",
       " 'storm': 26,\n",
       " 'home': 27,\n",
       " 'say': 28,\n",
       " 'crash': 29,\n",
       " 'burning': 30,\n",
       " 'attack': 31,\n",
       " 'got': 32,\n",
       " 'first': 33,\n",
       " 'know': 34,\n",
       " 'suicide': 35,\n",
       " 'family': 36,\n",
       " 'world': 37,\n",
       " 'back': 38,\n",
       " 'make': 39,\n",
       " 'car': 40,\n",
       " 'california': 41,\n",
       " 'rt': 42,\n",
       " 'train': 43,\n",
       " 'flood': 44,\n",
       " 'today': 45,\n",
       " 'see': 46,\n",
       " 'love': 47,\n",
       " 'bomb': 48,\n",
       " 'nuclear': 49,\n",
       " 'death': 50,\n",
       " 'going': 51,\n",
       " 'hiroshima': 52,\n",
       " 'full': 53,\n",
       " 'look': 54,\n",
       " 'watch': 55,\n",
       " 'man': 56,\n",
       " 'two': 57,\n",
       " 'let': 58,\n",
       " 'service': 59,\n",
       " 'cant': 60,\n",
       " 'take': 61,\n",
       " 'pm': 62,\n",
       " 'youtube': 63,\n",
       " 'think': 64,\n",
       " 'war': 65,\n",
       " 'want': 66,\n",
       " 'dead': 67,\n",
       " 'last': 68,\n",
       " 'need': 69,\n",
       " 'accident': 70,\n",
       " 'way': 71,\n",
       " 'good': 72,\n",
       " 'weapon': 73,\n",
       " 'bombing': 74,\n",
       " 'killed': 75,\n",
       " 'many': 76,\n",
       " 'even': 77,\n",
       " 'wildfire': 78,\n",
       " 'could': 79,\n",
       " 'injury': 80,\n",
       " 'bag': 81,\n",
       " 'collapse': 82,\n",
       " 'right': 83,\n",
       " 'water': 84,\n",
       " 'plan': 85,\n",
       " 'may': 86,\n",
       " 'best': 87,\n",
       " 'city': 88,\n",
       " 'black': 89,\n",
       " 'help': 90,\n",
       " 'feel': 91,\n",
       " 'hot': 92,\n",
       " 'woman': 93,\n",
       " 'work': 94,\n",
       " 'come': 95,\n",
       " 'w': 96,\n",
       " 'forest': 97,\n",
       " 'live': 98,\n",
       " 'youre': 99,\n",
       " 'photo': 100,\n",
       " 'another': 101,\n",
       " 'read': 102,\n",
       " 'cause': 103,\n",
       " 'army': 104,\n",
       " 'never': 105,\n",
       " 'lol': 106,\n",
       " 'house': 107,\n",
       " 'hostage': 108,\n",
       " 'thing': 109,\n",
       " 'really': 110,\n",
       " 'school': 111,\n",
       " 'casualty': 112,\n",
       " 'fear': 113,\n",
       " 'much': 114,\n",
       " 'northern': 115,\n",
       " 'wave': 116,\n",
       " 'damage': 117,\n",
       " 'bomber': 118,\n",
       " 'state': 119,\n",
       " 'show': 120,\n",
       " 'great': 121,\n",
       " 'obama': 122,\n",
       " 'post': 123,\n",
       " 'earthquake': 124,\n",
       " 'please': 125,\n",
       " 'report': 126,\n",
       " 'japan': 127,\n",
       " 'th': 128,\n",
       " 'night': 129,\n",
       " 'wild': 130,\n",
       " 'old': 131,\n",
       " 'stop': 132,\n",
       " 'run': 133,\n",
       " 'boy': 134,\n",
       " 'fatality': 135,\n",
       " 'said': 136,\n",
       " 'set': 137,\n",
       " 'latest': 138,\n",
       " 'near': 139,\n",
       " 'change': 140,\n",
       " 'top': 141,\n",
       " 'fall': 142,\n",
       " 'migrant': 143,\n",
       " 'thunderstorm': 144,\n",
       " 'flame': 145,\n",
       " 'hope': 146,\n",
       " 'truck': 147,\n",
       " 'hit': 148,\n",
       " 'food': 149,\n",
       " 'wind': 150,\n",
       " 'call': 151,\n",
       " 'atomic': 152,\n",
       " 'fatal': 153,\n",
       " 'ever': 154,\n",
       " 'terrorist': 155,\n",
       " 'coming': 156,\n",
       " 'mh': 157,\n",
       " 'injured': 158,\n",
       " 'story': 159,\n",
       " 'content': 160,\n",
       " 'rain': 161,\n",
       " 'always': 162,\n",
       " 'oil': 163,\n",
       " 'weather': 164,\n",
       " 'siren': 165,\n",
       " 'thats': 166,\n",
       " 'getting': 167,\n",
       " 'severe': 168,\n",
       " 'evacuation': 169,\n",
       " 'heat': 170,\n",
       " 'warning': 171,\n",
       " 'wounded': 172,\n",
       " 'guy': 173,\n",
       " 'minute': 174,\n",
       " 'sound': 175,\n",
       " 'smoke': 176,\n",
       " 'game': 177,\n",
       " 'fan': 178,\n",
       " 'summer': 179,\n",
       " 'well': 180,\n",
       " 'found': 181,\n",
       " 'also': 182,\n",
       " 'market': 183,\n",
       " 'lightning': 184,\n",
       " 'without': 185,\n",
       " 'week': 186,\n",
       " 'refugee': 187,\n",
       " 'everyone': 188,\n",
       " 'official': 189,\n",
       " 'save': 190,\n",
       " 'bad': 191,\n",
       " 'girl': 192,\n",
       " 'ive': 193,\n",
       " 'next': 194,\n",
       " 'since': 195,\n",
       " 'st': 196,\n",
       " 'destroy': 197,\n",
       " 'end': 198,\n",
       " 'ill': 199,\n",
       " 'every': 200,\n",
       " 'hundred': 201,\n",
       " 'boat': 202,\n",
       " 'someone': 203,\n",
       " 'head': 204,\n",
       " 'cross': 205,\n",
       " 'keep': 206,\n",
       " 'little': 207,\n",
       " 'china': 208,\n",
       " 'baby': 209,\n",
       " 'gonna': 210,\n",
       " 'movie': 211,\n",
       " 'flooding': 212,\n",
       " 'liked': 213,\n",
       " 'update': 214,\n",
       " 'landslide': 215,\n",
       " 'explosion': 216,\n",
       " 'survived': 217,\n",
       " 'survivor': 218,\n",
       " 'oh': 219,\n",
       " 'attacked': 220,\n",
       " 'military': 221,\n",
       " 'spill': 222,\n",
       " 'loud': 223,\n",
       " 'saudi': 224,\n",
       " 'area': 225,\n",
       " 'natural': 226,\n",
       " 'face': 227,\n",
       " 'debris': 228,\n",
       " 'free': 229,\n",
       " 'thunder': 230,\n",
       " 'rescued': 231,\n",
       " 'he': 232,\n",
       " 'part': 233,\n",
       " 'eye': 234,\n",
       " 'rescue': 235,\n",
       " 'county': 236,\n",
       " 'legionnaire': 237,\n",
       " 'hail': 238,\n",
       " 'check': 239,\n",
       " 'thought': 240,\n",
       " 'put': 241,\n",
       " 'destroyed': 242,\n",
       " 'sign': 243,\n",
       " 'child': 244,\n",
       " 'panic': 245,\n",
       " 'terrorism': 246,\n",
       " 'bloody': 247,\n",
       " 'group': 248,\n",
       " 'national': 249,\n",
       " 'there': 250,\n",
       " 'road': 251,\n",
       " 'wound': 252,\n",
       " 'blood': 253,\n",
       " 'tonight': 254,\n",
       " 'responder': 255,\n",
       " 'drought': 256,\n",
       " 'around': 257,\n",
       " 'breaking': 258,\n",
       " 'failure': 259,\n",
       " 'bridge': 260,\n",
       " 'drowning': 261,\n",
       " 'inundated': 262,\n",
       " 'dust': 263,\n",
       " 'burned': 264,\n",
       " 'order': 265,\n",
       " 'phone': 266,\n",
       " 'issue': 267,\n",
       " 'stock': 268,\n",
       " 'air': 269,\n",
       " 'survive': 270,\n",
       " 'trapped': 271,\n",
       " 'yr': 272,\n",
       " 'heart': 273,\n",
       " 'rescuer': 274,\n",
       " 'calgary': 275,\n",
       " 'destruction': 276,\n",
       " 'lava': 277,\n",
       " 'screaming': 278,\n",
       " 'whole': 279,\n",
       " 'wreck': 280,\n",
       " 'ok': 281,\n",
       " 'away': 282,\n",
       " 'job': 283,\n",
       " 'big': 284,\n",
       " 'island': 285,\n",
       " 'saw': 286,\n",
       " 'cliff': 287,\n",
       " 'detonation': 288,\n",
       " 'heard': 289,\n",
       " 'p': 290,\n",
       " 'start': 291,\n",
       " 'effect': 292,\n",
       " 'second': 293,\n",
       " 'made': 294,\n",
       " 'trauma': 295,\n",
       " 'hurricane': 296,\n",
       " 'derailment': 297,\n",
       " 'mosque': 298,\n",
       " 'horrible': 299,\n",
       " 'collided': 300,\n",
       " 'blast': 301,\n",
       " 'power': 302,\n",
       " 'red': 303,\n",
       " 'riot': 304,\n",
       " 'white': 305,\n",
       " 'seismic': 306,\n",
       " 'apocalypse': 307,\n",
       " 'hour': 308,\n",
       " 'kill': 309,\n",
       " 'officer': 310,\n",
       " 'bleeding': 311,\n",
       " 'bus': 312,\n",
       " 'reddit': 313,\n",
       " 'catastrophic': 314,\n",
       " 'use': 315,\n",
       " 'curfew': 316,\n",
       " 'blew': 317,\n",
       " 'lady': 318,\n",
       " 'quarantined': 319,\n",
       " 'demolished': 320,\n",
       " 'airport': 321,\n",
       " 'sandstorm': 322,\n",
       " 'missing': 323,\n",
       " 'kid': 324,\n",
       " 'hazardous': 325,\n",
       " 'trouble': 326,\n",
       " 'screamed': 327,\n",
       " 'sinking': 328,\n",
       " 'collision': 329,\n",
       " 'violent': 330,\n",
       " 'street': 331,\n",
       " 'suspect': 332,\n",
       " 'hazard': 333,\n",
       " 'bang': 334,\n",
       " 'released': 335,\n",
       " 'structural': 336,\n",
       " 'site': 337,\n",
       " 'obliterated': 338,\n",
       " 'song': 339,\n",
       " 'devastation': 340,\n",
       " 'charged': 341,\n",
       " 'remember': 342,\n",
       " 'tell': 343,\n",
       " 'due': 344,\n",
       " 'collapsed': 345,\n",
       " 'deal': 346,\n",
       " 'battle': 347,\n",
       " 'must': 348,\n",
       " 'business': 349,\n",
       " 'better': 350,\n",
       " 'light': 351,\n",
       " 'x': 352,\n",
       " 'traumatised': 353,\n",
       " 'ambulance': 354,\n",
       " 'obliteration': 355,\n",
       " 'famine': 356,\n",
       " 'volcano': 357,\n",
       " 'engulfed': 358,\n",
       " 'collide': 359,\n",
       " 'bombed': 360,\n",
       " 'shot': 361,\n",
       " 'person': 362,\n",
       " 'anniversary': 363,\n",
       " 'lot': 364,\n",
       " 'book': 365,\n",
       " 'play': 366,\n",
       " 'rubble': 367,\n",
       " 'ruin': 368,\n",
       " 'typhoon': 369,\n",
       " 'tragedy': 370,\n",
       " 'windstorm': 371,\n",
       " 'hijack': 372,\n",
       " 'august': 373,\n",
       " 'whirlwind': 374,\n",
       " 'crashed': 375,\n",
       " 'exploded': 376,\n",
       " 'armageddon': 377,\n",
       " 'ebay': 378,\n",
       " 'evacuate': 379,\n",
       " 'yet': 380,\n",
       " 'zone': 381,\n",
       " 'sinkhole': 382,\n",
       " 'wrecked': 383,\n",
       " 'word': 384,\n",
       " 'drown': 385,\n",
       " 'desolation': 386,\n",
       " 'airplane': 387,\n",
       " 'didnt': 388,\n",
       " 'river': 389,\n",
       " 'scream': 390,\n",
       " 'drowned': 391,\n",
       " 'past': 392,\n",
       " 'friend': 393,\n",
       " 'chemical': 394,\n",
       " 'died': 395,\n",
       " 'used': 396,\n",
       " 'snowstorm': 397,\n",
       " 'least': 398,\n",
       " 'american': 399,\n",
       " 'confirmed': 400,\n",
       " 'malaysia': 401,\n",
       " 'meltdown': 402,\n",
       " 'something': 403,\n",
       " 'iran': 404,\n",
       " 'catastrophe': 405,\n",
       " 'crush': 406,\n",
       " 'explode': 407,\n",
       " 'demolish': 408,\n",
       " 'blown': 409,\n",
       " 'real': 410,\n",
       " 'doesnt': 411,\n",
       " 'stretcher': 412,\n",
       " 'hijacking': 413,\n",
       " 'arson': 414,\n",
       " 'caused': 415,\n",
       " 'government': 416,\n",
       " 'quarantine': 417,\n",
       " 'sure': 418,\n",
       " 'obliterate': 419,\n",
       " 'case': 420,\n",
       " 'already': 421,\n",
       " 'tomorrow': 422,\n",
       " 'sunk': 423,\n",
       " 'twister': 424,\n",
       " 'shoulder': 425,\n",
       " 'panicking': 426,\n",
       " 'razed': 427,\n",
       " 'eyewitness': 428,\n",
       " 'annihilated': 429,\n",
       " 'danger': 430,\n",
       " 'tornado': 431,\n",
       " 'evacuated': 432,\n",
       " 'came': 433,\n",
       " 'fedex': 434,\n",
       " 'demolition': 435,\n",
       " 'isi': 436,\n",
       " 'medium': 437,\n",
       " 'pandemonium': 438,\n",
       " 'displaced': 439,\n",
       " 'bagging': 440,\n",
       " 'security': 441,\n",
       " 'send': 442,\n",
       " 'longer': 443,\n",
       " 'half': 444,\n",
       " 'trying': 445,\n",
       " 'mudslide': 446,\n",
       " 'ablaze': 447,\n",
       " 'men': 448,\n",
       " 'line': 449,\n",
       " 'avalanche': 450,\n",
       " 'rainstorm': 451,\n",
       " 'dog': 452,\n",
       " 'mayhem': 453,\n",
       " 'left': 454,\n",
       " 'blazing': 455,\n",
       " 'hijacker': 456,\n",
       " 'flattened': 457,\n",
       " 'electrocute': 458,\n",
       " 'crushed': 459,\n",
       " 'detonate': 460,\n",
       " 'tsunami': 461,\n",
       " 'three': 462,\n",
       " 'land': 463,\n",
       " 'n': 464,\n",
       " 'v': 465,\n",
       " 'deluged': 466,\n",
       " 'actually': 467,\n",
       " 'id': 468,\n",
       " 'place': 469,\n",
       " 'went': 470,\n",
       " 'cool': 471,\n",
       " 'shooting': 472,\n",
       " 'reason': 473,\n",
       " 'derailed': 474,\n",
       " 'far': 475,\n",
       " 'b': 476,\n",
       " 'rioting': 477,\n",
       " 'traffic': 478,\n",
       " 'high': 479,\n",
       " 'stay': 480,\n",
       " 'long': 481,\n",
       " 'fight': 482,\n",
       " 'wreckage': 483,\n",
       " 'investigator': 484,\n",
       " 'km': 485,\n",
       " 'hailstorm': 486,\n",
       " 'horror': 487,\n",
       " 'nothing': 488,\n",
       " 'electrocuted': 489,\n",
       " 'derail': 490,\n",
       " 'soon': 491,\n",
       " 'cyclone': 492,\n",
       " 'israel': 493,\n",
       " 'problem': 494,\n",
       " 'die': 495,\n",
       " 'rd': 496,\n",
       " 'india': 497,\n",
       " 'murderer': 498,\n",
       " 'bioterrorism': 499,\n",
       " 'swallowed': 500,\n",
       " 'music': 501,\n",
       " 'making': 502,\n",
       " 'bioterror': 503,\n",
       " 'harm': 504,\n",
       " 'computer': 505,\n",
       " 'west': 506,\n",
       " 'pic': 507,\n",
       " 'victim': 508,\n",
       " 'ur': 509,\n",
       " 'america': 510,\n",
       " 'twitter': 511,\n",
       " 'detonated': 512,\n",
       " 'outbreak': 513,\n",
       " 'macre': 514,\n",
       " 'north': 515,\n",
       " 'tweet': 516,\n",
       " 'caught': 517,\n",
       " 'mark': 518,\n",
       " 'wanna': 519,\n",
       " 'plane': 520,\n",
       " 'deluge': 521,\n",
       " 'fun': 522,\n",
       " 'though': 523,\n",
       " 'memory': 524,\n",
       " 'crew': 525,\n",
       " 'ago': 526,\n",
       " 'give': 527,\n",
       " 'done': 528,\n",
       " 'thanks': 529,\n",
       " 'prebreak': 530,\n",
       " 'pkk': 531,\n",
       " 'blight': 532,\n",
       " 'find': 533,\n",
       " 'bush': 534,\n",
       " 'abc': 535,\n",
       " 'upheaval': 536,\n",
       " 'team': 537,\n",
       " 'mean': 538,\n",
       " 'health': 539,\n",
       " 'level': 540,\n",
       " 'open': 541,\n",
       " 'brown': 542,\n",
       " 'data': 543,\n",
       " 'policy': 544,\n",
       " 'affected': 545,\n",
       " 'annihilation': 546,\n",
       " 'move': 547,\n",
       " 'turkey': 548,\n",
       " 'possible': 549,\n",
       " 'care': 550,\n",
       " 'pretty': 551,\n",
       " 'try': 552,\n",
       " 'rise': 553,\n",
       " 'might': 554,\n",
       " 'south': 555,\n",
       " 'wake': 556,\n",
       " 'support': 557,\n",
       " 'desolate': 558,\n",
       " 'nearby': 559,\n",
       " 'country': 560,\n",
       " 'pick': 561,\n",
       " 'wait': 562,\n",
       " 'aftershock': 563,\n",
       " 'australia': 564,\n",
       " 'militant': 565,\n",
       " 'win': 566,\n",
       " 'side': 567,\n",
       " 'fukushima': 568,\n",
       " 'hellfire': 569,\n",
       " 'firefighter': 570,\n",
       " 'giant': 571,\n",
       " 'park': 572,\n",
       " 'pay': 573,\n",
       " 'theyre': 574,\n",
       " 'emmerdale': 575,\n",
       " 'feared': 576,\n",
       " 'almost': 577,\n",
       " 'mp': 578,\n",
       " 'reunion': 579,\n",
       " 'yeah': 580,\n",
       " 'yearold': 581,\n",
       " 'horse': 582,\n",
       " 'feeling': 583,\n",
       " 'star': 584,\n",
       " 'king': 585,\n",
       " 'reactor': 586,\n",
       " 'town': 587,\n",
       " 'crazy': 588,\n",
       " 'aug': 589,\n",
       " 'point': 590,\n",
       " 'moment': 591,\n",
       " 'center': 592,\n",
       " 'gem': 593,\n",
       " 'hear': 594,\n",
       " 'low': 595,\n",
       " 'hand': 596,\n",
       " 'reuters': 597,\n",
       " 'claim': 598,\n",
       " 'hate': 599,\n",
       " 'declares': 600,\n",
       " 'saipan': 601,\n",
       " 'israeli': 602,\n",
       " 'everything': 603,\n",
       " 'lab': 604,\n",
       " 'human': 605,\n",
       " 'literally': 606,\n",
       " 'talk': 607,\n",
       " 'wont': 608,\n",
       " 'yes': 609,\n",
       " 'seen': 610,\n",
       " 'watching': 611,\n",
       " 'anyone': 612,\n",
       " 'thank': 613,\n",
       " 'believe': 614,\n",
       " 'money': 615,\n",
       " 'listen': 616,\n",
       " 'piece': 617,\n",
       " 'outside': 618,\n",
       " 'vehicle': 619,\n",
       " 'beautiful': 620,\n",
       " 'bigger': 621,\n",
       " 'finally': 622,\n",
       " 'month': 623,\n",
       " 'control': 624,\n",
       " 'peace': 625,\n",
       " 'bar': 626,\n",
       " 'muslim': 627,\n",
       " 'inside': 628,\n",
       " 'bc': 629,\n",
       " 'tree': 630,\n",
       " 'spot': 631,\n",
       " 'isnt': 632,\n",
       " 'brother': 633,\n",
       " 'ship': 634,\n",
       " 'blaze': 635,\n",
       " 'couple': 636,\n",
       " 'pakistan': 637,\n",
       " 'huge': 638,\n",
       " 'r': 639,\n",
       " 'happened': 640,\n",
       " 'drake': 641,\n",
       " 'blizzard': 642,\n",
       " 'village': 643,\n",
       " 'entire': 644,\n",
       " 'idea': 645,\n",
       " 'whats': 646,\n",
       " 'knock': 647,\n",
       " 'c': 648,\n",
       " 'saved': 649,\n",
       " 'crisis': 650,\n",
       " 'nw': 651,\n",
       " 'hat': 652,\n",
       " 'searching': 653,\n",
       " 'maybe': 654,\n",
       " 'banned': 655,\n",
       " 'texas': 656,\n",
       " 'action': 657,\n",
       " 'transport': 658,\n",
       " 'k': 659,\n",
       " 'flag': 660,\n",
       " 'offensive': 661,\n",
       " 'handbag': 662,\n",
       " 'major': 663,\n",
       " 'gun': 664,\n",
       " 'public': 665,\n",
       " 'helicopter': 666,\n",
       " 'shift': 667,\n",
       " 'stand': 668,\n",
       " 'cover': 669,\n",
       " 'lie': 670,\n",
       " 'scene': 671,\n",
       " 'history': 672,\n",
       " 'thursday': 673,\n",
       " 'wow': 674,\n",
       " 'miss': 675,\n",
       " 'amid': 676,\n",
       " 'probably': 677,\n",
       " 'seek': 678,\n",
       " 'typhoondevastated': 679,\n",
       " 'member': 680,\n",
       " 'devastated': 681,\n",
       " 'morning': 682,\n",
       " 'share': 683,\n",
       " 'christian': 684,\n",
       " 'haha': 685,\n",
       " 'ball': 686,\n",
       " 'coach': 687,\n",
       " 'room': 688,\n",
       " 'mom': 689,\n",
       " 'cake': 690,\n",
       " 'film': 691,\n",
       " 'islam': 692,\n",
       " 'east': 693,\n",
       " 'rock': 694,\n",
       " 'happy': 695,\n",
       " 'course': 696,\n",
       " 'flight': 697,\n",
       " 'drive': 698,\n",
       " 'nowplaying': 699,\n",
       " 'bbc': 700,\n",
       " 'civilian': 701,\n",
       " 'ignition': 702,\n",
       " 'lost': 703,\n",
       " 'soudelor': 704,\n",
       " 'fact': 705,\n",
       " 'ancient': 706,\n",
       " 'favorite': 707,\n",
       " 'taken': 708,\n",
       " 'la': 709,\n",
       " 'earth': 710,\n",
       " 'trench': 711,\n",
       " 'nearly': 712,\n",
       " 'insurance': 713,\n",
       " 'called': 714,\n",
       " 'sue': 715,\n",
       " 'rule': 716,\n",
       " 'salt': 717,\n",
       " 'gt': 718,\n",
       " 'tv': 719,\n",
       " 'property': 720,\n",
       " 'burn': 721,\n",
       " 'season': 722,\n",
       " 'anything': 723,\n",
       " 'bestnaijamade': 724,\n",
       " 'islamic': 725,\n",
       " 'anthrax': 726,\n",
       " 'hey': 727,\n",
       " 'break': 728,\n",
       " 'hard': 729,\n",
       " 'cut': 730,\n",
       " 'across': 731,\n",
       " 'name': 732,\n",
       " 'projected': 733,\n",
       " 'appears': 734,\n",
       " 'galactic': 735,\n",
       " 'utc': 736,\n",
       " 'link': 737,\n",
       " 'subreddits': 738,\n",
       " 'art': 739,\n",
       " 'heavy': 740,\n",
       " 'flash': 741,\n",
       " 'space': 742,\n",
       " 'united': 743,\n",
       " 'build': 744,\n",
       " 'strong': 745,\n",
       " 'sorry': 746,\n",
       " 'mph': 747,\n",
       " 'large': 748,\n",
       " 'awesome': 749,\n",
       " 'arsonist': 750,\n",
       " 'answer': 751,\n",
       " 'threat': 752,\n",
       " 'womens': 753,\n",
       " 'truth': 754,\n",
       " 'cnn': 755,\n",
       " 'expert': 756,\n",
       " 'begin': 757,\n",
       " 'russian': 758,\n",
       " 'camp': 759,\n",
       " 'gbbo': 760,\n",
       " 'risk': 761,\n",
       " 'declaration': 762,\n",
       " 'else': 763,\n",
       " 'reddits': 764,\n",
       " 'release': 765,\n",
       " 'climate': 766,\n",
       " 'secret': 767,\n",
       " 'yesterday': 768,\n",
       " 'comment': 769,\n",
       " 'ice': 770,\n",
       " 'led': 771,\n",
       " 'issued': 772,\n",
       " 'green': 773,\n",
       " 'following': 774,\n",
       " 'event': 775,\n",
       " 'potus': 776,\n",
       " 'leather': 777,\n",
       " 'looking': 778,\n",
       " 'text': 779,\n",
       " 'nd': 780,\n",
       " 'enough': 781,\n",
       " 'daily': 782,\n",
       " 'hiring': 783,\n",
       " 'fast': 784,\n",
       " 'radio': 785,\n",
       " 'strike': 786,\n",
       " 'spring': 787,\n",
       " 'running': 788,\n",
       " 'cop': 789,\n",
       " 'refugio': 790,\n",
       " 'costlier': 791,\n",
       " 'force': 792,\n",
       " 'youth': 793,\n",
       " 'leave': 794,\n",
       " 'become': 795,\n",
       " 'unconfirmed': 796,\n",
       " 'neighbour': 797,\n",
       " 'bring': 798,\n",
       " 'funtenna': 799,\n",
       " 'washington': 800,\n",
       " 'cl': 801,\n",
       " 'quiz': 802,\n",
       " 'mishap': 803,\n",
       " 'usa': 804,\n",
       " 'million': 805,\n",
       " 'alarm': 806,\n",
       " 'myanmar': 807,\n",
       " 'centre': 808,\n",
       " 'mad': 809,\n",
       " 'track': 810,\n",
       " 'ca': 811,\n",
       " 'saying': 812,\n",
       " 'taking': 813,\n",
       " 'palestinian': 814,\n",
       " 'terror': 815,\n",
       " 'conclusively': 816,\n",
       " 'hero': 817,\n",
       " 'learn': 818,\n",
       " 'join': 819,\n",
       " 'worst': 820,\n",
       " 'seeing': 821,\n",
       " 'shes': 822,\n",
       " 'closed': 823,\n",
       " 'aint': 824,\n",
       " 'apollo': 825,\n",
       " 'safety': 826,\n",
       " 'picture': 827,\n",
       " 'poor': 828,\n",
       " 'hollywood': 829,\n",
       " 'blue': 830,\n",
       " 'picking': 831,\n",
       " 'toddler': 832,\n",
       " 'york': 833,\n",
       " 'others': 834,\n",
       " 'turn': 835,\n",
       " 'question': 836,\n",
       " 'bit': 837,\n",
       " 'escape': 838,\n",
       " 'follow': 839,\n",
       " 'wednesday': 840,\n",
       " 'chance': 841,\n",
       " 'aircraft': 842,\n",
       " 'manslaughter': 843,\n",
       " 'super': 844,\n",
       " 'fashion': 845,\n",
       " 'former': 846,\n",
       " 'date': 847,\n",
       " 'small': 848,\n",
       " 'account': 849,\n",
       " 'ppl': 850,\n",
       " 'true': 851,\n",
       " 'nigerian': 852,\n",
       " 'result': 853,\n",
       " 'likely': 854,\n",
       " 'capture': 855,\n",
       " 'japanese': 856,\n",
       " 'front': 857,\n",
       " 'block': 858,\n",
       " 'aba': 859,\n",
       " 'beat': 860,\n",
       " 'information': 861,\n",
       " 'gave': 862,\n",
       " 'global': 863,\n",
       " 'meek': 864,\n",
       " 'self': 865,\n",
       " 'mop': 866,\n",
       " 'suspected': 867,\n",
       " 'nagasaki': 868,\n",
       " 'germ': 869,\n",
       " 'cree': 870,\n",
       " 'thousand': 871,\n",
       " 'four': 872,\n",
       " 'g': 873,\n",
       " 'reported': 874,\n",
       " 'lord': 875,\n",
       " 'soul': 876,\n",
       " 'angry': 877,\n",
       " 'sport': 878,\n",
       " 'wrong': 879,\n",
       " 'local': 880,\n",
       " 'cry': 881,\n",
       " 'wonder': 882,\n",
       " 'london': 883,\n",
       " 'mount': 884,\n",
       " 'ash': 885,\n",
       " 'libya': 886,\n",
       " 'temple': 887,\n",
       " 'british': 888,\n",
       " 'behind': 889,\n",
       " 'number': 890,\n",
       " 'target': 891,\n",
       " 'tote': 892,\n",
       " 'double': 893,\n",
       " 'happening': 894,\n",
       " 'tried': 895,\n",
       " 'cost': 896,\n",
       " 'lead': 897,\n",
       " 'biggest': 898,\n",
       " 'ready': 899,\n",
       " 'worse': 900,\n",
       " 'young': 901,\n",
       " 'arent': 902,\n",
       " 'sky': 903,\n",
       " 'lake': 904,\n",
       " 'central': 905,\n",
       " 'student': 906,\n",
       " 'album': 907,\n",
       " 'dad': 908,\n",
       " 'living': 909,\n",
       " 'online': 910,\n",
       " 'court': 911,\n",
       " 'delivers': 912,\n",
       " 'udhampur': 913,\n",
       " 'rly': 914,\n",
       " 'madhya': 915,\n",
       " 'pradesh': 916,\n",
       " 'outrage': 917,\n",
       " 'mountain': 918,\n",
       " 'killing': 919,\n",
       " 'episode': 920,\n",
       " 'miner': 921,\n",
       " 'early': 922,\n",
       " 'direction': 923,\n",
       " 'foot': 924,\n",
       " 'marians': 925,\n",
       " 'door': 926,\n",
       " 'short': 927,\n",
       " 'thinking': 928,\n",
       " 'series': 929,\n",
       " 'training': 930,\n",
       " 'research': 931,\n",
       " 'upon': 932,\n",
       " 'pray': 933,\n",
       " 'wall': 934,\n",
       " 'wasnt': 935,\n",
       " 'giving': 936,\n",
       " 'playing': 937,\n",
       " 'e': 938,\n",
       " 'act': 939,\n",
       " 'guide': 940,\n",
       " 'lt': 941,\n",
       " 'return': 942,\n",
       " 'travel': 943,\n",
       " 'mine': 944,\n",
       " 'driving': 945,\n",
       " 'mile': 946,\n",
       " 'mind': 947,\n",
       " 'em': 948,\n",
       " 'totally': 949,\n",
       " 'colorado': 950,\n",
       " 'told': 951,\n",
       " 'company': 952,\n",
       " 'close': 953,\n",
       " 'board': 954,\n",
       " 'instead': 955,\n",
       " 'happen': 956,\n",
       " 'worry': 957,\n",
       " 'downtown': 958,\n",
       " 'interesting': 959,\n",
       " 'party': 960,\n",
       " 'ave': 961,\n",
       " 'richmond': 962,\n",
       " 'together': 963,\n",
       " 'internally': 964,\n",
       " 'unit': 965,\n",
       " 'wife': 966,\n",
       " 'dude': 967,\n",
       " 'theater': 968,\n",
       " 'ban': 969,\n",
       " 'remove': 970,\n",
       " 'rate': 971,\n",
       " 'absolutely': 972,\n",
       " 'dance': 973,\n",
       " 'view': 974,\n",
       " 'department': 975,\n",
       " 'broke': 976,\n",
       " 'either': 977,\n",
       " 'career': 978,\n",
       " 'pain': 979,\n",
       " 'okay': 980,\n",
       " 'player': 981,\n",
       " 'patience': 982,\n",
       " 'bayelsa': 983,\n",
       " 'owner': 984,\n",
       " 'michael': 985,\n",
       " 'record': 986,\n",
       " 'program': 987,\n",
       " 'ahead': 988,\n",
       " 'walk': 989,\n",
       " 'ebola': 990,\n",
       " 'browser': 991,\n",
       " 'character': 992,\n",
       " 'expected': 993,\n",
       " 'gone': 994,\n",
       " 'pengers': 995,\n",
       " 'smh': 996,\n",
       " 'ross': 997,\n",
       " 'sad': 998,\n",
       " 'middle': 999,\n",
       " 'landing': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22222"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check max len of sentences to cut if too long to optimize response time\n",
    "max_len = max(len(x) for x in X_train)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill vacant word as 0 from preorder\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y label chagne to categorical (one-hot encoding)\n",
    "y_train_seq = to_categorical(y_train_encoded)\n",
    "y_test_seq = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.5383 - loss: 0.7258 - precision_6: 0.5266 - recall_6: 0.3605 - val_accuracy: 0.7758 - val_loss: 0.4805 - val_precision_6: 0.7761 - val_recall_6: 0.7739\n",
      "Epoch 2/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.8451 - loss: 0.3805 - precision_6: 0.8427 - recall_6: 0.8477 - val_accuracy: 0.8278 - val_loss: 0.4225 - val_precision_6: 0.8264 - val_recall_6: 0.8287\n",
      "Epoch 3/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9424 - loss: 0.1637 - precision_6: 0.9420 - recall_6: 0.9426 - val_accuracy: 0.8177 - val_loss: 0.5032 - val_precision_6: 0.8170 - val_recall_6: 0.8181\n",
      "Epoch 4/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9832 - loss: 0.0579 - precision_6: 0.9830 - recall_6: 0.9831 - val_accuracy: 0.8080 - val_loss: 0.5536 - val_precision_6: 0.8082 - val_recall_6: 0.8089\n",
      "Epoch 5/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9917 - loss: 0.0292 - precision_6: 0.9920 - recall_6: 0.9919 - val_accuracy: 0.8034 - val_loss: 0.5995 - val_precision_6: 0.8030 - val_recall_6: 0.8052\n",
      "Epoch 6/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9925 - loss: 0.0235 - precision_6: 0.9924 - recall_6: 0.9925 - val_accuracy: 0.8099 - val_loss: 0.6305 - val_precision_6: 0.8083 - val_recall_6: 0.8094\n",
      "Epoch 7/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9959 - loss: 0.0139 - precision_6: 0.9957 - recall_6: 0.9959 - val_accuracy: 0.8062 - val_loss: 0.6925 - val_precision_6: 0.8062 - val_recall_6: 0.8062\n",
      "Epoch 8/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9943 - loss: 0.0205 - precision_6: 0.9943 - recall_6: 0.9943 - val_accuracy: 0.7901 - val_loss: 0.6676 - val_precision_6: 0.7883 - val_recall_6: 0.7905\n",
      "Epoch 9/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9941 - loss: 0.0188 - precision_6: 0.9941 - recall_6: 0.9942 - val_accuracy: 0.7887 - val_loss: 0.6845 - val_precision_6: 0.7889 - val_recall_6: 0.7882\n",
      "Epoch 10/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9947 - loss: 0.0142 - precision_6: 0.9947 - recall_6: 0.9947 - val_accuracy: 0.8002 - val_loss: 0.6960 - val_precision_6: 0.8003 - val_recall_6: 0.8006\n",
      "Epoch 11/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9948 - loss: 0.0156 - precision_6: 0.9948 - recall_6: 0.9948 - val_accuracy: 0.7873 - val_loss: 0.6904 - val_precision_6: 0.7877 - val_recall_6: 0.7873\n",
      "Epoch 12/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9950 - loss: 0.0148 - precision_6: 0.9950 - recall_6: 0.9950 - val_accuracy: 0.7753 - val_loss: 0.6952 - val_precision_6: 0.7767 - val_recall_6: 0.7721\n",
      "Epoch 13/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9947 - loss: 0.0150 - precision_6: 0.9947 - recall_6: 0.9947 - val_accuracy: 0.7951 - val_loss: 0.6999 - val_precision_6: 0.7934 - val_recall_6: 0.7956\n",
      "Epoch 14/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9949 - loss: 0.0143 - precision_6: 0.9947 - recall_6: 0.9949 - val_accuracy: 0.7956 - val_loss: 0.6985 - val_precision_6: 0.7943 - val_recall_6: 0.7965\n",
      "Epoch 15/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.9944 - loss: 0.0146 - precision_6: 0.9944 - recall_6: 0.9944 - val_accuracy: 0.7924 - val_loss: 0.7054 - val_precision_6: 0.7924 - val_recall_6: 0.7924\n",
      "Epoch 16/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9953 - loss: 0.0113 - precision_6: 0.9953 - recall_6: 0.9953 - val_accuracy: 0.7914 - val_loss: 0.7167 - val_precision_6: 0.7906 - val_recall_6: 0.7928\n",
      "Epoch 17/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.9950 - loss: 0.0119 - precision_6: 0.9952 - recall_6: 0.9950 - val_accuracy: 0.7831 - val_loss: 0.6985 - val_precision_6: 0.7831 - val_recall_6: 0.7831\n",
      "Epoch 18/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.9949 - loss: 0.0115 - precision_6: 0.9949 - recall_6: 0.9949 - val_accuracy: 0.7919 - val_loss: 0.7528 - val_precision_6: 0.7929 - val_recall_6: 0.7914\n",
      "Epoch 19/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step - accuracy: 0.9962 - loss: 0.0100 - precision_6: 0.9962 - recall_6: 0.9962 - val_accuracy: 0.7698 - val_loss: 0.7002 - val_precision_6: 0.7685 - val_recall_6: 0.7703\n",
      "Epoch 20/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.9956 - loss: 0.0095 - precision_6: 0.9956 - recall_6: 0.9956 - val_accuracy: 0.7689 - val_loss: 0.7029 - val_precision_6: 0.7696 - val_recall_6: 0.7689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30ccfcd60>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall  # 필요한 지표를 임포트\n",
    "\n",
    "\n",
    "model_dnn = Sequential()  # will be layered\n",
    "model_dnn.add(\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)\n",
    ")  # add 1 to adjust padding\n",
    "model_dnn.add(Dropout(0.2))  # prevent overfit\n",
    "model_dnn.add(Flatten())\n",
    "model_dnn.add(Dense(512, activation=\"relu\"))  # add dense layers as you want\n",
    "model_dnn.add(\n",
    "    Dense(len(y_train_seq[0]), activation=\"sigmoid\")\n",
    ")  # final dense layer should be sigmoid when binary category, else softmax\n",
    "\n",
    "model_dnn.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\", Precision(), Recall()],\n",
    ")\n",
    "\n",
    "model_dnn.fit(\n",
    "    X_train_pad, y_train_seq, epochs=20, batch_size=32, validation_data=(X_test_pad, y_test_seq)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출처:\n",
    "https://blog.eduonix.com/internet-of-things/simple-nlp-based-chatbot-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
